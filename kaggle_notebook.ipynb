{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ad20fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import joblib\n",
    "import traceback\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from scipy.stats import skew, kurtosis, SmallSampleWarning\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from tensorflow import shape, minimum\n",
    "from tensorflow.keras import backend as k\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import pad_sequences, Sequence, to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, Input, Conv1D, MaxPooling1D, GlobalAveragePooling1D, Concatenate,\n",
    "    BatchNormalization, GRU, Dropout, add, Activation, Multiply, Reshape,\n",
    "    LayerNormalization, Add, Bidirectional, LSTM, UpSampling1D, Lambda, GaussianNoise\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cbfadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_KAGGLE = os.getenv('KAGGLE_KERNEL_RUN_TYPE') is not None\n",
    "\n",
    "if IS_KAGGLE:\n",
    "    print(\"Running on Kaggle environment.\")\n",
    "    # Example paths for Kaggle - YOU WILL NEED TO ADJUST THESE\n",
    "    # Path to your pre-computed feature dataset\n",
    "    INPUT_DIR = Path(\"/kaggle/input/your-feature-dataset-name\") \n",
    "    # Path where artifacts (models, scalers) will be written\n",
    "    OUTPUT_DIR = Path(\"/kaggle/working/\")\n",
    "    # Path to the original raw competition data\n",
    "    RAW_DATA_DIR = Path(\"/kaggle/input/cmi-detect-behavior-with-sensor-data\")\n",
    "else:\n",
    "    print(\"Running on local environment.\")\n",
    "    # Adjust these paths to match your local project structure\n",
    "    INPUT_DIR = Path(\"output\") \n",
    "    OUTPUT_DIR = Path(\"output\")\n",
    "    RAW_DATA_DIR = Path(\"input/cmi-detect-behavior-with-sensor-data\")\n",
    "\n",
    "# --- File & Directory Paths ---\n",
    "PARQUET_FILE = INPUT_DIR / 'final_model_input_dataset.parquet' # Example name\n",
    "PRETRAINED_DIR = OUTPUT_DIR / \"artifacts\"\n",
    "PRETRAINED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Model & Training Hyperparameters ---\n",
    "LR_INIT = 5e-4\n",
    "WD = 3e-3\n",
    "NUM_CLASSES = 18\n",
    "BATCH_SIZE = 64\n",
    "N_SPLITS = 4 \n",
    "MAX_PAD_LEN = 128\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a785dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def se_block(x, reduction=8):\n",
    "    ch = x.shape[-1]\n",
    "    se = GlobalAveragePooling1D()(x)\n",
    "    se = Dense(ch // reduction, activation='relu')(se)\n",
    "    se = Dense(ch, activation='sigmoid')(se)\n",
    "    se = Reshape((1, ch))(se)\n",
    "    return Multiply()([x, se])\n",
    "\n",
    "def residual_se_cnn_block(x, filters, kernel_size, pool_size=2, drop=0.3, wd=1e-4):\n",
    "    shortcut = x\n",
    "    for _ in range(2):\n",
    "        x = Conv1D(filters, kernel_size, padding='same', use_bias=False, kernel_regularizer=l2(wd))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "    x = se_block(x)\n",
    "    if shortcut.shape[-1] != filters:\n",
    "        shortcut = Conv1D(filters, 1, padding='same', use_bias=False, kernel_regularizer=l2(wd))(shortcut)\n",
    "        shortcut = BatchNormalization()(shortcut)\n",
    "    x = add([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling1D(pool_size)(x)\n",
    "    x = Dropout(drop)(x)\n",
    "    return x\n",
    "\n",
    "def crop_or_pad(inputs):\n",
    "    x, skip = inputs\n",
    "    x_len = shape(x)[1]\n",
    "    skip_len = shape(skip)[1]\n",
    "    min_len = minimum(x_len, skip_len)\n",
    "    return x[:, :min_len, :], skip[:, :min_len, :]\n",
    "\n",
    "def crop_or_pad_output_shape(input_shapes):\n",
    "    shape1, shape2 = input_shapes\n",
    "    min_time_steps = min(shape1[1], shape2[1]) if shape1[1] is not None and shape2[1] is not None else None\n",
    "    num_features = shape1[2]\n",
    "    output_shape = (None, min_time_steps, num_features)\n",
    "    return [output_shape, output_shape]\n",
    "\n",
    "def match_time_steps(x, skip):    \n",
    "    x, skip = Lambda(crop_or_pad, output_shape=crop_or_pad_output_shape)([x, skip])\n",
    "    return x, skip\n",
    "\n",
    "def res_se_cnn_decoder_block(x, filters, kernel_size, drop=0.3, wd=1e-4, skip_connection=None):\n",
    "    x = UpSampling1D(size=2)(x)\n",
    "    x = Conv1D(filters, kernel_size, padding='same', use_bias=False, kernel_regularizer=l2(wd))(x)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    if skip_connection is not None:\n",
    "        x, skip_connection = match_time_steps(x, skip_connection)\n",
    "        x = Concatenate()([x, skip_connection])\n",
    "    x = Conv1D(filters, kernel_size, padding='same', use_bias=False, kernel_regularizer=l2(wd))(x)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = se_block(x)\n",
    "    x = Dropout(drop)(x)\n",
    "    return x\n",
    "\n",
    "def unet_se_cnn(x, unet_depth=3, base_filters=64, kernel_size=3, drop=0.3):\n",
    "    filters = base_filters\n",
    "    skips = []\n",
    "    for _ in range(unet_depth):\n",
    "        x = residual_se_cnn_block(x, filters, kernel_size, drop=drop)\n",
    "        skips.append(x)\n",
    "        filters *= 2\n",
    "    c_shape = x.shape[-1]\n",
    "    x = Dense(128)(x)\n",
    "    x = Dense(c_shape)(x)\n",
    "    for skip in reversed(skips):\n",
    "        filters //= 2\n",
    "        x = res_se_cnn_decoder_block(x, filters, kernel_size, drop=drop, skip_connection=skip)\n",
    "    return x\n",
    "\n",
    "def tof_block(tof_inputs, wd=1e-4):\n",
    "    x2_base = Conv1D(64, 3, padding='same', use_bias=False, kernel_regularizer=l2(wd))(tof_inputs)\n",
    "    x2_base = BatchNormalization()(x2_base); x2_base = Activation('relu')(x2_base)\n",
    "    x2_base = MaxPooling1D(2)(x2_base); x2_base = Dropout(0.2)(x2_base)\n",
    "    x2_base = Conv1D(128, 3, padding='same', use_bias=False, kernel_regularizer=l2(wd))(x2_base)\n",
    "    x2_base = BatchNormalization()(x2_base); x2_base = Activation('relu')(x2_base)\n",
    "    gate_input = GlobalAveragePooling1D()(tof_inputs)\n",
    "    gate_input = Dense(16, activation='relu')(gate_input)\n",
    "    gate = Dense(1, activation='sigmoid', name='tof_gate_dense')(gate_input)\n",
    "    return Multiply()([x2_base, gate])\n",
    "\n",
    "def time_sum(x): return k.sum(x, axis=1)\n",
    "def squeeze_last_axis(x): return tf.squeeze(x, axis=-1)\n",
    "def expand_last_axis(x): return tf.expand_dims(x, axis=-1)\n",
    "\n",
    "def attention_layer(inputs):\n",
    "    score = Dense(1, activation='tanh')(inputs)\n",
    "    score = Lambda(squeeze_last_axis)(score)\n",
    "    weights = Activation('softmax')(score)\n",
    "    weights = Lambda(expand_last_axis)(weights)\n",
    "    context = Multiply()([inputs, weights])\n",
    "    context = Lambda(time_sum)(context)\n",
    "    return context    \n",
    "\n",
    "def features_processing(x1, x2, wd=1e-4):\n",
    "    x1_matched, x2_matched = match_time_steps(x1, x2)\n",
    "    merged = Concatenate()([x1_matched, x2_matched])\n",
    "    xa = Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer=l2(wd)))(merged)\n",
    "    xb = Bidirectional(GRU(128, return_sequences=True, kernel_regularizer=l2(wd)))(merged)\n",
    "    xc = GaussianNoise(0.09)(merged)\n",
    "    xc = Dense(16, activation='elu')(xc)\n",
    "    x = Concatenate()([xa, xb, xc])\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = attention_layer(x)\n",
    "    for units, drop in [(256, 0.5), (128, 0.3)]:\n",
    "        x = Dense(units, use_bias=False, kernel_regularizer=l2(wd))(x)\n",
    "        x = BatchNormalization()(x); x = Activation('relu')(x)\n",
    "        x = Dropout(drop)(x)\n",
    "    return x\n",
    "\n",
    "# =====================================================================================\n",
    "# SEGMENT 4: GENERAL HELPER FUNCTIONS\n",
    "# =====================================================================================\n",
    "\n",
    "class GatedMixupGenerator(Sequence):\n",
    "    def __init__(self, X, y, gate_targets, batch_size, imu_dim, class_weight=None, alpha=0.2, masking_prob=0.0):\n",
    "        self.X, self.y = X, y\n",
    "        self.gate_targets = gate_targets  \n",
    "        self.batch = batch_size\n",
    "        self.imu_dim = imu_dim\n",
    "        self.class_weight = class_weight\n",
    "        self.alpha = alpha\n",
    "        self.masking_prob = masking_prob\n",
    "        self.indices = np.arange(len(X))\n",
    "        self.on_epoch_end()\n",
    "    def __len__(self): return int(np.ceil(len(self.X) / self.batch))\n",
    "    def __getitem__(self, i):\n",
    "        idx = self.indices[i*self.batch:(i+1)*self.batch]\n",
    "        Xb, yb = self.X[idx].copy(), self.y[idx].copy()\n",
    "        gate_target = self.gate_targets[idx].copy()\n",
    "        if self.masking_prob > 0:\n",
    "            for sample_idx in range(len(Xb)):\n",
    "                if gate_target[sample_idx] == 1.0 and np.random.rand() < self.masking_prob:\n",
    "                    Xb[sample_idx, :, self.imu_dim:] = 0\n",
    "                    gate_target[sample_idx] = 0.0\n",
    "        sample_weights = np.ones(len(Xb), dtype='float32')\n",
    "        if self.class_weight:\n",
    "            y_integers = yb.argmax(axis=1)\n",
    "            sample_weights = np.array([self.class_weight[i] for i in y_integers])\n",
    "        if self.alpha > 0:\n",
    "            lam = np.random.beta(self.alpha, self.alpha)\n",
    "            perm = np.random.permutation(len(Xb))\n",
    "            X_mix = lam * Xb + (1 - lam) * Xb[perm]\n",
    "            y_mix = lam * yb + (1 - lam) * yb[perm]\n",
    "            gate_target_mix = lam * gate_target + (1 - lam) * gate_target[perm]\n",
    "            sample_weights_mix = lam * sample_weights + (1 - lam) * sample_weights[perm]\n",
    "            return X_mix, {'main_output': y_mix, 'tof_gate': gate_target_mix[:, np.newaxis]}, sample_weights_mix\n",
    "        return Xb, {'main_output': yb, 'tof_gate': gate_target[:, np.newaxis]}, sample_weights    \n",
    "    def on_epoch_end(self): np.random.shuffle(self.indices)\n",
    "\n",
    "def create_sequence_dataset(df: pl.DataFrame, feature_cols: list, gate_df: pl.DataFrame):\n",
    "    sequences, labels, gate_targets = [], [], []\n",
    "    df_with_gate = df.join(gate_df, on='sequence_id', how='left')\n",
    "    for seq_id, group in df_with_gate.group_by('sequence_id', maintain_order=True):\n",
    "        sequences.append(group.select(feature_cols).to_numpy())\n",
    "        labels.append(group.select('gesture_int').item(0, 0))\n",
    "        gate_targets.append(group.select('has_tof').item(0, 0))\n",
    "    return np.array(sequences, dtype=object), np.array(labels), np.array(gate_targets)\n",
    "\n",
    "def train_model(model, train_dataset, val_dataset, epochs, initial_learning_rate, weight_decay):\n",
    "    optimizer = tf.keras.optimizers.AdamW(learning_rate=initial_learning_rate, weight_decay=weight_decay)\n",
    "    early_stopping = EarlyStopping(monitor='val_main_output_accuracy', patience=20, restore_best_weights=True, mode='max')\n",
    "    model.compile(optimizer=optimizer, loss={'main_output': 'categorical_crossentropy', 'tof_gate': 'binary_crossentropy'},\n",
    "                  loss_weights={'main_output': 1.0, 'tof_gate': 0.5}, metrics={\"main_output\": \"accuracy\"})\n",
    "    model.fit(train_dataset, validation_data=val_dataset, epochs=epochs, callbacks=[early_stopping])\n",
    "\n",
    "def generate_gate_targets(df: pl.DataFrame, tof_cols: list) -> pl.DataFrame:\n",
    "    gate_df = df.group_by(\"sequence_id\").agg(\n",
    "        pl.any_horizontal(pl.col(tof_cols).is_not_null().any()).alias(\"has_tof\")\n",
    "    )\n",
    "    return gate_df.with_columns(pl.col(\"has_tof\").cast(pl.Float32))    \n",
    "\n",
    "def perform_padding(X, pad_len):\n",
    "    return pad_sequences(\n",
    "        X,\n",
    "        maxlen=pad_len,\n",
    "        padding='post', # Pad at the end of the sequence\n",
    "        dtype='float32',# Data type of the output tensor\n",
    "        truncating='post',\n",
    "        value=0.0,      # Value to use for padding (e.g., 0 for numerical data)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9599daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_gravity_polars(acc_df: pl.DataFrame, rot_df: pl.DataFrame) -> np.ndarray:\n",
    "    acc_values = acc_df.select(['acc_x', 'acc_y', 'acc_z']).to_numpy()\n",
    "    quat_values = rot_df.select(['rot_x', 'rot_y', 'rot_z', 'rot_w']).to_numpy()\n",
    "    num_samples = acc_values.shape[0]\n",
    "    linear_accel = np.zeros_like(acc_values)\n",
    "    gravity_world = np.array([0, 0, 9.81])\n",
    "    for i in range(num_samples):\n",
    "        if np.all(np.isnan(quat_values[i])) or np.all(np.isclose(quat_values[i], 0)):\n",
    "            linear_accel[i] = acc_values[i]\n",
    "            continue\n",
    "        try:\n",
    "            rotation = R.from_quat(quat_values[i])\n",
    "            gravity_sensor_frame = rotation.apply(gravity_world, inverse=True)\n",
    "            linear_accel[i] = acc_values[i] - gravity_sensor_frame\n",
    "        except ValueError:\n",
    "            linear_accel[i] = acc_values[i]\n",
    "    return linear_accel\n",
    "\n",
    "def calculate_angular_velocity(rot_df: pl.DataFrame, sampling_rate_hz: int) -> np.ndarray:\n",
    "    quats = rot_df.select(['rot_x', 'rot_y', 'rot_z', 'rot_w']).to_numpy()\n",
    "    angular_velocity = np.zeros_like(quats[:, :3])\n",
    "    dt = 1.0 / sampling_rate_hz\n",
    "    for i in range(1, len(quats)):\n",
    "        try:\n",
    "            q1 = R.from_quat(quats[i - 1])\n",
    "            q2 = R.from_quat(quats[i])\n",
    "            q_delta = q2 * q1.inv()\n",
    "            rot_vec = q_delta.as_rotvec()\n",
    "            angular_velocity[i] = rot_vec / dt\n",
    "        except ValueError:\n",
    "            angular_velocity[i] = 0\n",
    "    return angular_velocity\n",
    "\n",
    "def calculate_angular_acceleration(angular_velocity: np.ndarray, sampling_rate_hz: int) -> np.ndarray:\n",
    "    angular_accel = np.zeros_like(angular_velocity)\n",
    "    dt = 1.0 / sampling_rate_hz\n",
    "    angular_accel[1:] = np.diff(angular_velocity, axis=0) / dt\n",
    "    return angular_accel\n",
    "\n",
    "def calculate_gravity_orientation(rot_df: pl.DataFrame) -> np.ndarray:\n",
    "    quat_values = rot_df.select(['rot_x', 'rot_y', 'rot_z', 'rot_w']).to_numpy()\n",
    "    num_samples = quat_values.shape[0]\n",
    "    orientation_angles = np.zeros((num_samples, 3))\n",
    "    gravity_world = np.array([0, 0, 1.0])\n",
    "    for i in range(num_samples):\n",
    "        if np.all(np.isnan(quat_values[i])) or np.all(np.isclose(quat_values[i], 0)):\n",
    "            continue\n",
    "        try:\n",
    "            rotation = R.from_quat(quat_values[i])\n",
    "            sensor_axes_world = rotation.apply(np.eye(3))\n",
    "            for j in range(3):\n",
    "                dot_product = np.dot(sensor_axes_world[j], gravity_world)\n",
    "                orientation_angles[i, j] = np.arccos(np.clip(dot_product, -1.0, 1.0))\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return orientation_angles\n",
    "\n",
    "def pl_skew(s: pl.Series) -> float:\n",
    "    values = s.to_numpy()\n",
    "    if len(values) < 3: return None\n",
    "    if np.std(values) < 1e-9: return 0.0\n",
    "    return skew(values)\n",
    "\n",
    "def pl_kurtosis(s: pl.Series) -> float:\n",
    "    values = s.to_numpy()\n",
    "    if len(values) < 4: return None\n",
    "    if np.std(values) < 1e-9: return 0.0\n",
    "    return kurtosis(values)\n",
    "\n",
    "def process_tof_features(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    # (The full Polars-based ToF feature generation function goes here)\n",
    "    # ...\n",
    "    return df # Placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cff27b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================================\n",
    "# MASTER CONTROL FLAG\n",
    "# =====================================================================================\n",
    "TRAIN = False\n",
    "TRAIN = True \n",
    "\n",
    "# =====================================================================================\n",
    "# MODEL DEFINITION \n",
    "# =====================================================================================\n",
    "def create_model(dataset, imu_dim, wd=1e-4):\n",
    "    sample_batch = next(iter(dataset))\n",
    "    input_shape = sample_batch[0].shape[1:]\n",
    "    inp = tf.keras.layers.Input(shape=input_shape)\n",
    "    imu = tf.keras.layers.Lambda(lambda t: t[:, :, :imu_dim])(inp)\n",
    "    tof = tf.keras.layers.Lambda(lambda t: t[:, :, imu_dim:])(inp)\n",
    "\n",
    "    x1 = unet_se_cnn(imu, 3, base_filters=64, kernel_size=3)\n",
    "    x2 = tof_block(tof, wd)\n",
    "\n",
    "    x = features_processing(x1, x2)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x) \n",
    "    main_out = tf.keras.layers.Dense(18, activation=\"softmax\", name=\"main_output\")(x)\n",
    "    gate_out = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"tof_gate\")(x) \n",
    "    \n",
    "    return tf.keras.models.Model(inputs=inp, outputs={\"main_output\": main_out, \"tof_gate\": gate_out})\n",
    "\n",
    "# =====================================================================================\n",
    "# TRAINING LOGIC\n",
    "# =====================================================================================\n",
    "if TRAIN:\n",
    "    schema_df = pl.read_parquet(PARQUET_FILE, n_rows=0)\n",
    "    all_columns = schema_df.columns\n",
    "    meta_cols = {'gesture', 'gesture_int', 'sequence_type', 'behavior', 'orientation',\n",
    "                    'row_id', 'subject', 'phase', 'sequence_id', 'sequence_counter'}\n",
    "    feature_cols = [c for c in all_columns if c not in meta_cols]\n",
    "    imu_cols  = [c for c in feature_cols if not (c.startswith('thm_') or c.startswith('tof_'))]\n",
    "    tof_cols  = [c for c in feature_cols if c.startswith('thm_') or c.startswith('tof_')]\n",
    "\n",
    "    print(\"Scanning Parquet file for sequence IDs...\")\n",
    "    all_sequence_ids = (\n",
    "        pl.scan_parquet(PARQUET_FILE)\n",
    "        .select('sequence_id')\n",
    "        .unique()\n",
    "        .collect()\n",
    "        .to_numpy()\n",
    "        .ravel()\n",
    "    )\n",
    "    print(f\"Found {len(all_sequence_ids)} unique sequences.\")\n",
    "\n",
    "    kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "    fold_accuracies = []\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    imu_dim = len(imu_cols)\n",
    "\n",
    "    for fold_idx, (train_indices, val_indices) in enumerate(kf.split(all_sequence_ids)):\n",
    "        print(f\"\\n=== Fold {fold_idx + 1}/{N_SPLITS} ===\")\n",
    "        train_ids = all_sequence_ids[train_indices]\n",
    "        val_ids = all_sequence_ids[val_indices]\n",
    "\n",
    "        print(f\"Loading data for fold {fold_idx + 1}...\")\n",
    "        train_df = pl.read_parquet(PARQUET_FILE).filter(pl.col('sequence_id').is_in(train_ids))\n",
    "        val_df = pl.read_parquet(PARQUET_FILE).filter(pl.col('sequence_id').is_in(val_ids))\n",
    "        print(\"Fold data loaded.\")\n",
    "\n",
    "        train_gate_df = generate_gate_targets(train_df, tof_cols)\n",
    "        val_gate_df = generate_gate_targets(val_df, tof_cols)\n",
    "\n",
    "        le = LabelEncoder()\n",
    "        le.fit(train_df['gesture'])\n",
    "        train_df = train_df.with_columns(pl.Series(\"gesture_int\", le.transform(train_df['gesture'])))\n",
    "        val_df = val_df.with_columns(pl.Series(\"gesture_int\", le.transform(val_df['gesture'])))\n",
    "\n",
    "        # --- StandardScaler Logic ---\n",
    "        scaler = StandardScaler()\n",
    "        # Fit on training data and transform both\n",
    "        train_features_scaled = scaler.fit_transform(train_df[imu_cols + tof_cols])\n",
    "        val_features_scaled = scaler.transform(val_df[imu_cols + tof_cols])\n",
    "        # Create Polars DataFrames from the scaled numpy arrays\n",
    "        X_train_scaled_features = pl.DataFrame(train_features_scaled, schema=imu_cols + tof_cols)\n",
    "        X_val_scaled_features = pl.DataFrame(val_features_scaled, schema=imu_cols + tof_cols)\n",
    "\n",
    "        meta_cols_to_keep = ['sequence_id', 'gesture_int']\n",
    "        train_df_final = train_df.select(meta_cols_to_keep).with_columns(X_train_scaled_features)\n",
    "        val_df_final = val_df.select(meta_cols_to_keep).with_columns(X_val_scaled_features)\n",
    "\n",
    "        del train_df, val_df, X_train_scaled_features, X_val_scaled_features\n",
    "        gc.collect()\n",
    "\n",
    "        X_train, y_train, train_gate_target = create_sequence_dataset(train_df_final, imu_cols + tof_cols, train_gate_df)\n",
    "        X_val, y_val, val_gate_target = create_sequence_dataset(val_df_final, imu_cols + tof_cols, val_gate_df)\n",
    "\n",
    "        del train_df_final, val_df_final\n",
    "        gc.collect()\n",
    "\n",
    "        X_train_padded = perform_padding(X_train, MAX_PAD_LEN)\n",
    "        X_val_padded = perform_padding(X_val, MAX_PAD_LEN)\n",
    "        \n",
    "        y_train_cat = to_categorical(y_train, num_classes=NUM_CLASSES)\n",
    "        y_val_cat = to_categorical(y_val, num_classes=NUM_CLASSES)\n",
    "\n",
    "        train_dataset = GatedMixupGenerator(\n",
    "            X=X_train_padded, y=y_train_cat, gate_targets=train_gate_target,\n",
    "            batch_size=BATCH_SIZE, imu_dim=imu_dim, alpha=0.2, masking_prob=0.25\n",
    "        )\n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "            X_val_padded, {'main_output': y_val_cat, 'tof_gate': val_gate_target[:, np.newaxis]}\n",
    "        )).batch(BATCH_SIZE).cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        del X_val, y_val, X_train, y_train, X_train_padded, X_val_padded\n",
    "        gc.collect()\n",
    "        \n",
    "        model = create_model(train_dataset, len(imu_cols))\n",
    "        train_model(model, train_dataset, val_dataset, 150, LR_INIT, WD)\n",
    "\n",
    "        # --- SAVE ARTIFACTS ---\n",
    "        print(f\"--- Saving artifacts for Fold {fold_idx + 1} ---\")\n",
    "        model.save(PRETRAINED_DIR / f\"gesture_model_fold_{fold_idx}.h5\")\n",
    "        \n",
    "        # Save scaler and other metadata only from the first fold\n",
    "        if fold_idx == 0:\n",
    "            joblib.dump(scaler, PRETRAINED_DIR / \"scaler.pkl\")\n",
    "            np.save(PRETRAINED_DIR / \"feature_cols.npy\", np.array(imu_cols + tof_cols))\n",
    "            np.save(PRETRAINED_DIR / \"sequence_maxlen.npy\", MAX_PAD_LEN)\n",
    "            np.save(PRETRAINED_DIR / \"gesture_classes.npy\", le.classes_)\n",
    "            print(\"Scaler, feature_cols, maxlen, and classes saved.\")\n",
    "\n",
    "        # --- EVALUATION ---\n",
    "        val_preds = model.predict(val_dataset)\n",
    "        main_output_preds = val_preds['main_output']\n",
    "        y_pred_fold = np.argmax(main_output_preds, axis=1)\n",
    "        y_true_fold = np.argmax(y_val_cat, axis=1)\n",
    "        fold_acc = accuracy_score(y_true_fold, y_pred_fold)\n",
    "        fold_accuracies.append(fold_acc)\n",
    "        print(f\"Fold {fold_idx + 1} Accuracy: {fold_acc:.4f}\")\n",
    "        all_preds.append(y_pred_fold)\n",
    "        all_labels.append(y_true_fold)\n",
    "\n",
    "        del train_dataset, model, val_dataset\n",
    "        gc.collect()\n",
    "\n",
    "    # --- FINAL OOF REPORT ---\n",
    "    print(\"\\n=== Cross-validation Summary ===\")\n",
    "    print(f\"Per-fold Accuracies: {fold_accuracies}\")\n",
    "    print(f\"Mean Accuracy: {np.mean(fold_accuracies):.4f} ± {np.std(fold_accuracies):.4f}\")\n",
    "    y_all_pred = np.concatenate(all_preds)\n",
    "    y_all_true = np.concatenate(all_labels)\n",
    "    print(\"\\n=== Overall Classification Report ===\")\n",
    "    print(classification_report(y_all_true, y_all_pred, target_names=le.classes_, digits=4))\n",
    "\n",
    "# =====================================================================================\n",
    "# INFERENCE LOGIC\n",
    "# =====================================================================================\n",
    "else:\n",
    "    import pandas as pd \n",
    "    from src.metric import CompetitionMetric \n",
    "    from tensorflow import argmax, minimum, shape\n",
    "\n",
    "    def crop_or_pad(inputs):\n",
    "        x, skip = inputs\n",
    "        x_len = shape(x)[1]\n",
    "        skip_len = shape(skip)[1]\n",
    "        min_len = minimum(x_len, skip_len)\n",
    "        return x[:, :min_len, :], skip[:, :min_len, :]\n",
    "    \n",
    "    # --- 1. Load All Inference Artifacts ---\n",
    "    print(\"▶ INFERENCE MODE – loading artefacts from\", PRETRAINED_DIR)\n",
    "    final_feature_cols = np.load(PRETRAINED_DIR / \"feature_cols.npy\", allow_pickle=True).tolist()\n",
    "    pad_len = int(np.load(PRETRAINED_DIR / \"sequence_maxlen.npy\"))\n",
    "    scaler = joblib.load(PRETRAINED_DIR / \"scaler.pkl\")\n",
    "    gesture_classes = np.load(PRETRAINED_DIR / \"gesture_classes.npy\", allow_pickle=True)\n",
    "\n",
    "    print(\"Scaler expects:\", list(scaler.feature_names_in_))\n",
    "    print(\"We have:\", final_feature_cols)\n",
    "    print(\"Missing:\", set(scaler.feature_names_in_) - set(final_feature_cols))\n",
    "    print(\"Extra:\", set(final_feature_cols) - set(scaler.feature_names_in_))\n",
    "\n",
    "    models = []\n",
    "    print(f\"  Loading {N_SPLITS} models for ensemble inference...\")\n",
    "    for fold in range(N_SPLITS):\n",
    "        model_path = PRETRAINED_DIR / f\"gesture_model_fold_{fold}.h5\"\n",
    "        \n",
    "        model = load_model(model_path, compile=False, custom_objects={\n",
    "            'unet_se_cnn': unet_se_cnn,\n",
    "            'tof_block': tof_block,\n",
    "            'features_processing': features_processing,\n",
    "            'match_time_steps': match_time_steps,\n",
    "            'crop_or_pad': crop_or_pad,\n",
    "            'squeeze_last_axis': squeeze_last_axis,\n",
    "            'expand_last_axis': expand_last_axis,\n",
    "            'time_sum': time_sum,\n",
    "            'crop_or_pad_output_shape': crop_or_pad_output_shape\n",
    "        })\n",
    "        models.append(model)\n",
    "    print(\"  Models, scaler, and metadata loaded – ready for evaluation.\")\n",
    "\n",
    "    # --- 2. Define TTA Parameters and Predict Function ---\n",
    "    TTA_STEPS = 10\n",
    "    TTA_NOISE_STDDEV = 0.01\n",
    "\n",
    "    from src.tof_feats import remove_gravity_from_acc, calculate_angular_velocity_from_quat, calculate_angular_distance\n",
    "\n",
    "    def predict(sequence: pl.DataFrame, demographics: pl.DataFrame) -> str:\n",
    "        # Convert to pandas for the processing pipeline\n",
    "        df_seq = sequence.to_pandas()\n",
    "\n",
    "        # =================================================================================\n",
    "        # --- Step 1: Sanitize Raw Inputs ---\n",
    "        # =================================================================================\n",
    "        # This is a robust guard against non-numeric data in the hidden test set.\n",
    "        sensor_cols = [c for c in df_seq.columns if c.startswith(('acc_', 'rot_', 'thm_', 'tof_'))]\n",
    "        for col in sensor_cols:\n",
    "            if df_seq[col].dtype == 'object':\n",
    "                df_seq[col] = pd.to_numeric(df_seq[col], errors='coerce')\n",
    "\n",
    "        # =================================================================================\n",
    "        # --- Step 2: Feature Engineering (Must match training pipeline exactly) ---\n",
    "        # =================================================================================\n",
    "        # Create features in a dictionary to avoid fragmenting the DataFrame.\n",
    "        new_features = {}\n",
    "\n",
    "        # --- IMU Features ---\n",
    "        linear_accel = remove_gravity_from_acc(df_seq, df_seq)\n",
    "        new_features['linear_acc_x'] = linear_accel[:, 0]\n",
    "        new_features['linear_acc_y'] = linear_accel[:, 1]\n",
    "        new_features['linear_acc_z'] = linear_accel[:, 2]\n",
    "        \n",
    "        linear_acc_mag = np.sqrt(np.square(linear_accel).sum(axis=1))\n",
    "        new_features['linear_acc_mag'] = linear_acc_mag\n",
    "        new_features['linear_acc_mag_jerk'] = pd.Series(linear_acc_mag).diff().fillna(0).values\n",
    "        \n",
    "        angular_vel = calculate_angular_velocity_from_quat(df_seq)\n",
    "        new_features['angular_vel_x'] = angular_vel[:, 0]\n",
    "        new_features['angular_vel_y'] = angular_vel[:, 1]\n",
    "        new_features['angular_vel_z'] = angular_vel[:, 2]\n",
    "        \n",
    "        new_features['angular_distance'] = calculate_angular_distance(df_seq)\n",
    "\n",
    "        # --- ToF Aggregated Features ---\n",
    "        # This is the part that was missing.\n",
    "        for i in range(1, 6):\n",
    "            pixel_cols = [f\"tof_{i}_v{p}\" for p in range(64)]\n",
    "            # The underlying tof_v... columns are already sanitized.\n",
    "            # We still replace -1 as it's a specific code, not bad data.\n",
    "            tof_data = df_seq[pixel_cols].replace(-1, np.nan) \n",
    "            new_features[f'tof_{i}_mean'] = tof_data.mean(axis=1)\n",
    "            new_features[f'tof_{i}_std'] = tof_data.std(axis=1)\n",
    "            new_features[f'tof_{i}_min'] = tof_data.min(axis=1)\n",
    "            new_features[f'tof_{i}_max'] = tof_data.max(axis=1)\n",
    "\n",
    "        # Add all new features to the DataFrame in one efficient operation\n",
    "        df_seq = df_seq.assign(**new_features)\n",
    "\n",
    "        # =================================================================================\n",
    "        # --- Step 3: Final Processing (Scaling and Padding) ---\n",
    "        # =================================================================================\n",
    "\n",
    "        mat_unscaled_df = df_seq[final_feature_cols]\n",
    "        mat_unscaled_filled = mat_unscaled_df.ffill().bfill().fillna(0)\n",
    "        mat_scaled = scaler.transform(mat_unscaled_filled)\n",
    "        pad_input = pad_sequences([mat_scaled], maxlen=pad_len, padding='post', truncating='post', dtype='float32')\n",
    "\n",
    "        # =================================================================================\n",
    "        # --- Step 4: TTA and Ensemble Prediction ---\n",
    "        # =================================================================================\n",
    "        all_tta_predictions = []\n",
    "        for i in range(TTA_STEPS):\n",
    "            noisy_input = pad_input\n",
    "            if i > 0:\n",
    "                noise = tf.random.normal(shape=tf.shape(pad_input), mean=0.0, stddev=TTA_NOISE_STDDEV)\n",
    "                noisy_input = pad_input + noise\n",
    "\n",
    "            all_fold_predictions = []\n",
    "            for model in models:\n",
    "                # Correctly unpack the dictionary returned by the model\n",
    "                predictions_dict = model.predict(noisy_input, verbose=0)\n",
    "                main_preds = predictions_dict['main_output']\n",
    "                all_fold_predictions.append(main_preds)\n",
    "            \n",
    "            avg_fold_prediction = np.mean(all_fold_predictions, axis=0)\n",
    "            all_tta_predictions.append(avg_fold_prediction)\n",
    "\n",
    "        # =================================================================================\n",
    "        # --- Step 5: Final Averaging and Prediction ---\n",
    "        # =================================================================================\n",
    "        final_avg_prediction = np.mean(all_tta_predictions, axis=0)\n",
    "        idx = int(final_avg_prediction.argmax())\n",
    "        \n",
    "        return str(gesture_classes[idx])\n",
    "    \n",
    "    # --- 3. Run Kaggle Evaluation Server ---\n",
    "    import kaggle_evaluation.cmi_inference_server\n",
    "    inference_server = kaggle_evaluation.cmi_inference_server.CMIInferenceServer(predict)\n",
    "\n",
    "    if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "        inference_server.serve()\n",
    "    else:\n",
    "        # For local testing, you need to provide the paths to the test data\n",
    "        print(\"Running local gateway for testing...\")\n",
    "        inference_server.run_local_gateway(\n",
    "            data_paths=(\n",
    "                'input/cmi-detect-behavior-with-sensor-data/test.csv',\n",
    "                'input/cmi-detect-behavior-with-sensor-data/test_demographics.csv',\n",
    "            )\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
