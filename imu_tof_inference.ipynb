{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea175bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ INFERENCE MODE – loading artefacts from output/artifacts\n",
      "Scaler expects: ['linear_acc_x', 'linear_acc_y', 'linear_acc_z', 'rot_w', 'rot_x', 'rot_y', 'rot_z', 'linear_acc_mag', 'linear_acc_mag_jerk', 'angular_vel_x', 'angular_vel_y', 'angular_vel_z', 'angular_distance', 'thm_1', 'thm_2', 'thm_3', 'thm_4', 'thm_5', 'tof_1_mean', 'tof_1_std', 'tof_1_min', 'tof_1_max', 'tof_2_mean', 'tof_2_std', 'tof_2_min', 'tof_2_max', 'tof_3_mean', 'tof_3_std', 'tof_3_min', 'tof_3_max', 'tof_4_mean', 'tof_4_std', 'tof_4_min', 'tof_4_max', 'tof_5_mean', 'tof_5_std', 'tof_5_min', 'tof_5_max']\n",
      "We have: ['linear_acc_x', 'linear_acc_y', 'linear_acc_z', 'rot_w', 'rot_x', 'rot_y', 'rot_z', 'linear_acc_mag', 'linear_acc_mag_jerk', 'angular_vel_x', 'angular_vel_y', 'angular_vel_z', 'angular_distance', 'thm_1', 'thm_2', 'thm_3', 'thm_4', 'thm_5', 'tof_1_mean', 'tof_1_std', 'tof_1_min', 'tof_1_max', 'tof_2_mean', 'tof_2_std', 'tof_2_min', 'tof_2_max', 'tof_3_mean', 'tof_3_std', 'tof_3_min', 'tof_3_max', 'tof_4_mean', 'tof_4_std', 'tof_4_min', 'tof_4_max', 'tof_5_mean', 'tof_5_std', 'tof_5_min', 'tof_5_max']\n",
      "Missing: set()\n",
      "Extra: set()\n",
      "  Loading 4 models for ensemble inference...\n",
      "  Models, scaler, and metadata loaded – ready for evaluation.\n",
      "Running local gateway for testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1754822045.790024  302509 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import joblib\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import pad_sequences, to_categorical\n",
    "\n",
    "from src.nn_blocks import (\n",
    "    unet_se_cnn,\n",
    "    features_processing, \n",
    "    GatedMixupGenerator, \n",
    "    tof_block, \n",
    "    match_time_steps, \n",
    "    time_sum, \n",
    "    squeeze_last_axis,\n",
    "    expand_last_axis,\n",
    "    crop_or_pad_output_shape\n",
    ")\n",
    "\n",
    "from src.functions import (\n",
    "    train_model, \n",
    "    create_sequence_dataset,\n",
    "    perform_padding,\n",
    "    generate_gate_targets\n",
    ")\n",
    "from src.constants import DATA_PATH\n",
    "\n",
    "# =====================================================================================\n",
    "# MASTER CONTROL FLAG\n",
    "# =====================================================================================\n",
    "TRAIN = True \n",
    "TRAIN = False\n",
    "\n",
    "# =====================================================================================\n",
    "# CONFIGURATION\n",
    "# =====================================================================================\n",
    "PARQUET_FILE = 'data/tof_kaggle_feats.parquet'\n",
    "PRETRAINED_DIR = Path(\"output/artifacts\")\n",
    "PRETRAINED_DIR.mkdir(parents=True, exist_ok=True) # Ensure directory exists\n",
    "\n",
    "LR_INIT = 5e-4\n",
    "WD = 3e-3\n",
    "NUM_CLASSES = 18\n",
    "BATCH_SIZE = 64\n",
    "N_SPLITS = 4 \n",
    "MAX_PAD_LEN = 128\n",
    "\n",
    "# =====================================================================================\n",
    "# MODEL DEFINITION (Your existing function)\n",
    "# =====================================================================================\n",
    "def create_model(dataset, imu_dim, wd=1e-4):\n",
    "    sample_batch = next(iter(dataset))\n",
    "    input_shape = sample_batch[0].shape[1:]\n",
    "    inp = tf.keras.layers.Input(shape=input_shape)\n",
    "    imu = tf.keras.layers.Lambda(lambda t: t[:, :, :imu_dim])(inp)\n",
    "    tof = tf.keras.layers.Lambda(lambda t: t[:, :, imu_dim:])(inp)\n",
    "\n",
    "    x1 = unet_se_cnn(imu, 3, base_filters=64, kernel_size=3)\n",
    "    x2 = tof_block(tof, wd)\n",
    "\n",
    "    x = features_processing(x1, x2)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x) \n",
    "    main_out = tf.keras.layers.Dense(18, activation=\"softmax\", name=\"main_output\")(x)\n",
    "    gate_out = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"tof_gate\")(x) # Renamed layer\n",
    "    \n",
    "    return tf.keras.models.Model(inputs=inp, outputs={\"main_output\": main_out, \"tof_gate\": gate_out})\n",
    "\n",
    "# =====================================================================================\n",
    "# TRAINING LOGIC\n",
    "# =====================================================================================\n",
    "if TRAIN:\n",
    "    schema_df = pl.read_parquet(PARQUET_FILE, n_rows=0)\n",
    "    all_columns = schema_df.columns\n",
    "    meta_cols = {'gesture', 'gesture_int', 'sequence_type', 'behavior', 'orientation',\n",
    "                    'row_id', 'subject', 'phase', 'sequence_id', 'sequence_counter'}\n",
    "    feature_cols = [c for c in all_columns if c not in meta_cols]\n",
    "    imu_cols  = [c for c in feature_cols if not (c.startswith('thm_') or c.startswith('tof_'))]\n",
    "    tof_cols  = [c for c in feature_cols if c.startswith('thm_') or c.startswith('tof_')]\n",
    "\n",
    "    print(\"Scanning Parquet file for sequence IDs...\")\n",
    "    all_sequence_ids = (\n",
    "        pl.scan_parquet(PARQUET_FILE)\n",
    "        .select('sequence_id')\n",
    "        .unique()\n",
    "        .collect()\n",
    "        .to_numpy()\n",
    "        .ravel()\n",
    "    )\n",
    "    print(f\"Found {len(all_sequence_ids)} unique sequences.\")\n",
    "\n",
    "    kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "    fold_accuracies = []\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    imu_dim = len(imu_cols)\n",
    "\n",
    "    for fold_idx, (train_indices, val_indices) in enumerate(kf.split(all_sequence_ids)):\n",
    "        print(f\"\\n=== Fold {fold_idx + 1}/{N_SPLITS} ===\")\n",
    "        train_ids = all_sequence_ids[train_indices]\n",
    "        val_ids = all_sequence_ids[val_indices]\n",
    "\n",
    "        print(f\"Loading data for fold {fold_idx + 1}...\")\n",
    "        train_df = pl.read_parquet(PARQUET_FILE).filter(pl.col('sequence_id').is_in(train_ids))\n",
    "        val_df = pl.read_parquet(PARQUET_FILE).filter(pl.col('sequence_id').is_in(val_ids))\n",
    "        print(\"Fold data loaded.\")\n",
    "\n",
    "        train_gate_df = generate_gate_targets(train_df, tof_cols)\n",
    "        val_gate_df = generate_gate_targets(val_df, tof_cols)\n",
    "\n",
    "        le = LabelEncoder()\n",
    "        le.fit(train_df['gesture'])\n",
    "        train_df = train_df.with_columns(pl.Series(\"gesture_int\", le.transform(train_df['gesture'])))\n",
    "        val_df = val_df.with_columns(pl.Series(\"gesture_int\", le.transform(val_df['gesture'])))\n",
    "\n",
    "        # --- StandardScaler Logic ---\n",
    "        scaler = StandardScaler()\n",
    "        # Fit on training data and transform both\n",
    "        train_features_scaled = scaler.fit_transform(train_df[imu_cols + tof_cols])\n",
    "        val_features_scaled = scaler.transform(val_df[imu_cols + tof_cols])\n",
    "        # Create Polars DataFrames from the scaled numpy arrays\n",
    "        X_train_scaled_features = pl.DataFrame(train_features_scaled, schema=imu_cols + tof_cols)\n",
    "        X_val_scaled_features = pl.DataFrame(val_features_scaled, schema=imu_cols + tof_cols)\n",
    "\n",
    "        meta_cols_to_keep = ['sequence_id', 'gesture_int']\n",
    "        train_df_final = train_df.select(meta_cols_to_keep).with_columns(X_train_scaled_features)\n",
    "        val_df_final = val_df.select(meta_cols_to_keep).with_columns(X_val_scaled_features)\n",
    "\n",
    "        del train_df, val_df, X_train_scaled_features, X_val_scaled_features\n",
    "        gc.collect()\n",
    "\n",
    "        X_train, y_train, train_gate_target = create_sequence_dataset(train_df_final, imu_cols + tof_cols, train_gate_df)\n",
    "        X_val, y_val, val_gate_target = create_sequence_dataset(val_df_final, imu_cols + tof_cols, val_gate_df)\n",
    "\n",
    "        del train_df_final, val_df_final\n",
    "        gc.collect()\n",
    "\n",
    "        X_train_padded = perform_padding(X_train, MAX_PAD_LEN)\n",
    "        X_val_padded = perform_padding(X_val, MAX_PAD_LEN)\n",
    "        \n",
    "        y_train_cat = to_categorical(y_train, num_classes=NUM_CLASSES)\n",
    "        y_val_cat = to_categorical(y_val, num_classes=NUM_CLASSES)\n",
    "\n",
    "        train_dataset = GatedMixupGenerator(\n",
    "            X=X_train_padded, y=y_train_cat, gate_targets=train_gate_target,\n",
    "            batch_size=BATCH_SIZE, imu_dim=imu_dim, alpha=0.2, masking_prob=0.25\n",
    "        )\n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "            X_val_padded, {'main_output': y_val_cat, 'tof_gate': val_gate_target[:, np.newaxis]}\n",
    "        )).batch(BATCH_SIZE).cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        del X_val, y_val, X_train, y_train, X_train_padded, X_val_padded\n",
    "        gc.collect()\n",
    "        \n",
    "        model = create_model(train_dataset, len(imu_cols))\n",
    "        train_model(model, train_dataset, val_dataset, 150, LR_INIT, WD)\n",
    "\n",
    "        # --- SAVE ARTIFACTS ---\n",
    "        print(f\"--- Saving artifacts for Fold {fold_idx + 1} ---\")\n",
    "        model.save(PRETRAINED_DIR / f\"gesture_model_fold_{fold_idx}.h5\")\n",
    "        \n",
    "        # Save scaler and other metadata only from the first fold\n",
    "        if fold_idx == 0:\n",
    "            joblib.dump(scaler, PRETRAINED_DIR / \"scaler.pkl\")\n",
    "            np.save(PRETRAINED_DIR / \"feature_cols.npy\", np.array(imu_cols + tof_cols))\n",
    "            np.save(PRETRAINED_DIR / \"sequence_maxlen.npy\", MAX_PAD_LEN)\n",
    "            np.save(PRETRAINED_DIR / \"gesture_classes.npy\", le.classes_)\n",
    "            print(\"Scaler, feature_cols, maxlen, and classes saved.\")\n",
    "\n",
    "        # --- EVALUATION ---\n",
    "        val_preds = model.predict(val_dataset)\n",
    "        main_output_preds = val_preds['main_output']\n",
    "        y_pred_fold = np.argmax(main_output_preds, axis=1)\n",
    "        y_true_fold = np.argmax(y_val_cat, axis=1)\n",
    "        fold_acc = accuracy_score(y_true_fold, y_pred_fold)\n",
    "        fold_accuracies.append(fold_acc)\n",
    "        print(f\"Fold {fold_idx + 1} Accuracy: {fold_acc:.4f}\")\n",
    "        all_preds.append(y_pred_fold)\n",
    "        all_labels.append(y_true_fold)\n",
    "\n",
    "        del train_dataset, model, val_dataset\n",
    "        gc.collect()\n",
    "\n",
    "    # --- FINAL OOF REPORT ---\n",
    "    print(\"\\n=== Cross-validation Summary ===\")\n",
    "    print(f\"Per-fold Accuracies: {fold_accuracies}\")\n",
    "    print(f\"Mean Accuracy: {np.mean(fold_accuracies):.4f} ± {np.std(fold_accuracies):.4f}\")\n",
    "    y_all_pred = np.concatenate(all_preds)\n",
    "    y_all_true = np.concatenate(all_labels)\n",
    "    print(\"\\n=== Overall Classification Report ===\")\n",
    "    print(classification_report(y_all_true, y_all_pred, target_names=le.classes_, digits=4))\n",
    "\n",
    "# =====================================================================================\n",
    "# INFERENCE LOGIC\n",
    "# =====================================================================================\n",
    "else:\n",
    "    import pandas as pd \n",
    "    from src.metric import CompetitionMetric \n",
    "    from tensorflow import argmax, minimum, shape\n",
    "\n",
    "    def crop_or_pad(inputs):\n",
    "        x, skip = inputs\n",
    "        x_len = shape(x)[1]\n",
    "        skip_len = shape(skip)[1]\n",
    "        min_len = minimum(x_len, skip_len)\n",
    "        return x[:, :min_len, :], skip[:, :min_len, :]\n",
    "    \n",
    "    # --- 1. Load All Inference Artifacts ---\n",
    "    print(\"▶ INFERENCE MODE – loading artefacts from\", PRETRAINED_DIR)\n",
    "    final_feature_cols = np.load(PRETRAINED_DIR / \"feature_cols.npy\", allow_pickle=True).tolist()\n",
    "    pad_len = int(np.load(PRETRAINED_DIR / \"sequence_maxlen.npy\"))\n",
    "    scaler = joblib.load(PRETRAINED_DIR / \"scaler.pkl\")\n",
    "    gesture_classes = np.load(PRETRAINED_DIR / \"gesture_classes.npy\", allow_pickle=True)\n",
    "\n",
    "    print(\"Scaler expects:\", list(scaler.feature_names_in_))\n",
    "    print(\"We have:\", final_feature_cols)\n",
    "    print(\"Missing:\", set(scaler.feature_names_in_) - set(final_feature_cols))\n",
    "    print(\"Extra:\", set(final_feature_cols) - set(scaler.feature_names_in_))\n",
    "\n",
    "    models = []\n",
    "    print(f\"  Loading {N_SPLITS} models for ensemble inference...\")\n",
    "    for fold in range(N_SPLITS):\n",
    "        model_path = PRETRAINED_DIR / f\"gesture_model_fold_{fold}.h5\"\n",
    "        # Note: You only need to pass custom LAYERS/FUNCTIONS here, not the whole model definition\n",
    "        model = load_model(model_path, compile=False, custom_objects={\n",
    "            'unet_se_cnn': unet_se_cnn,\n",
    "            'tof_block': tof_block,\n",
    "            'features_processing': features_processing,\n",
    "            'match_time_steps': match_time_steps,\n",
    "            'crop_or_pad': crop_or_pad,\n",
    "            'squeeze_last_axis': squeeze_last_axis,\n",
    "            'expand_last_axis': expand_last_axis,\n",
    "            'time_sum': time_sum,\n",
    "            'crop_or_pad_output_shape': crop_or_pad_output_shape\n",
    "        })\n",
    "        models.append(model)\n",
    "    print(\"  Models, scaler, and metadata loaded – ready for evaluation.\")\n",
    "\n",
    "    # --- 2. Define TTA Parameters and Predict Function ---\n",
    "    TTA_STEPS = 10\n",
    "    TTA_NOISE_STDDEV = 0.01\n",
    "\n",
    "    from src.tof_feats import remove_gravity_from_acc, calculate_angular_velocity_from_quat, calculate_angular_distance\n",
    "\n",
    "    def predict(sequence: pl.DataFrame, demographics: pl.DataFrame) -> str:\n",
    "        # ... (All your feature engineering code is correct and can remain the same) ...\n",
    "        df_seq = sequence.to_pandas()\n",
    "        # ... (Sanitization, feature creation, scaling, padding) ...\n",
    "        sensor_cols = [c for c in df_seq.columns if c.startswith(('acc_', 'rot_', 'thm_', 'tof_'))]\n",
    "        for col in sensor_cols:\n",
    "            if df_seq[col].dtype == 'object':\n",
    "                df_seq[col] = pd.to_numeric(df_seq[col], errors='coerce')\n",
    "        new_features = {}\n",
    "        linear_accel = remove_gravity_from_acc(df_seq, df_seq)\n",
    "        new_features['linear_acc_x'] = linear_accel[:, 0]\n",
    "        new_features['linear_acc_y'] = linear_accel[:, 1]\n",
    "        new_features['linear_acc_z'] = linear_accel[:, 2]\n",
    "        linear_acc_mag = np.sqrt(np.square(linear_accel).sum(axis=1))\n",
    "        new_features['linear_acc_mag'] = linear_acc_mag\n",
    "        new_features['linear_acc_mag_jerk'] = pd.Series(linear_acc_mag).diff().fillna(0).values\n",
    "        angular_vel = calculate_angular_velocity_from_quat(df_seq)\n",
    "        new_features['angular_vel_x'] = angular_vel[:, 0]\n",
    "        new_features['angular_vel_y'] = angular_vel[:, 1]\n",
    "        new_features['angular_vel_z'] = angular_vel[:, 2]\n",
    "        new_features['angular_distance'] = calculate_angular_distance(df_seq)\n",
    "        for i in range(1, 6):\n",
    "            pixel_cols = [f\"tof_{i}_v{p}\" for p in range(64)]\n",
    "            tof_data = df_seq[pixel_cols].replace(-1, np.nan)\n",
    "            new_features[f'tof_{i}_mean'] = tof_data.mean(axis=1)\n",
    "            new_features[f'tof_{i}_std'] = tof_data.std(axis=1)\n",
    "            new_features[f'tof_{i}_min'] = tof_data.min(axis=1)\n",
    "            new_features[f'tof_{i}_max'] = tof_data.max(axis=1)\n",
    "        df_seq = df_seq.assign(**new_features)\n",
    "        mat_unscaled_df = df_seq[final_feature_cols].ffill().bfill().fillna(0)\n",
    "        mat_scaled = scaler.transform(mat_unscaled_df)\n",
    "        pad_input = pad_sequences([mat_scaled], maxlen=pad_len, padding='post', truncating='post', dtype='float32')\n",
    "\n",
    "        # --- TTA Loop ---\n",
    "        all_tta_predictions = []\n",
    "        for i in range(TTA_STEPS):\n",
    "            noisy_input = pad_input\n",
    "            if i > 0:\n",
    "                noise = tf.random.normal(shape=tf.shape(pad_input), mean=0.0, stddev=TTA_NOISE_STDDEV)\n",
    "                noisy_input = pad_input + noise\n",
    "\n",
    "            # Ensemble predictions from all fold models\n",
    "            all_fold_predictions = []\n",
    "            for model in models:\n",
    "                \n",
    "                # =========================================================================\n",
    "                # --- THE FINAL FIX IS HERE ---\n",
    "                # =========================================================================\n",
    "                # model.predict returns a dictionary, access the 'main_output' key\n",
    "                predictions_dict = model.predict(noisy_input, verbose=0)\n",
    "                main_preds = predictions_dict['main_output']\n",
    "                \n",
    "                all_fold_predictions.append(main_preds)\n",
    "            \n",
    "            avg_fold_prediction = np.mean(all_fold_predictions, axis=0)\n",
    "            all_tta_predictions.append(avg_fold_prediction)\n",
    "\n",
    "        # --- Final Averaging and Prediction (Unchanged) ---\n",
    "        final_avg_prediction = np.mean(all_tta_predictions, axis=0)\n",
    "        idx = int(final_avg_prediction.argmax())\n",
    "        \n",
    "        return str(gesture_classes[idx])\n",
    "    \n",
    "    # --- 3. Run Kaggle Evaluation Server ---\n",
    "    import kaggle_evaluation.cmi_inference_server\n",
    "    inference_server = kaggle_evaluation.cmi_inference_server.CMIInferenceServer(predict)\n",
    "\n",
    "    if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "        inference_server.serve()\n",
    "    else:\n",
    "        # For local testing, you need to provide the paths to the test data\n",
    "        print(\"Running local gateway for testing...\")\n",
    "        inference_server.run_local_gateway(\n",
    "            data_paths=(\n",
    "                'input/cmi-detect-behavior-with-sensor-data/test.csv',\n",
    "                'input/cmi-detect-behavior-with-sensor-data/test_demographics.csv',\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e24c3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ LOCAL DEBUG MODE – loading artefacts from output/artifacts\n",
      "  Loading 4 models for ensemble inference...\n",
      "  Models, scaler, and metadata loaded.\n",
      "\n",
      "--- Starting Local Test ---\n",
      "Testing with sequence_id: SEQ_000001\n",
      "\n",
      "Calling predict function directly...\n",
      "\n",
      "✅ SUCCESS! The function ran without errors on a sample.\n",
      "Predicted Gesture: Eyelash - pull hair\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # =====================================================================================\n",
    "# # --- INFERENCE & LOCAL DEBUGGING SCRIPT ---\n",
    "# # =====================================================================================\n",
    "# import pandas as pd\n",
    "# import polars as pl\n",
    "# import numpy as np\n",
    "# import joblib\n",
    "# import traceback\n",
    "# from tensorflow.keras.models import load_model\n",
    "# from tensorflow.keras.utils import pad_sequences\n",
    "\n",
    "# import os\n",
    "# import gc\n",
    "# import joblib\n",
    "# import numpy as np\n",
    "# import polars as pl\n",
    "# import tensorflow as tf\n",
    "# from pathlib import Path\n",
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "# from sklearn.metrics import classification_report, accuracy_score\n",
    "# from tensorflow.keras.models import load_model\n",
    "# from tensorflow.keras.utils import pad_sequences, to_categorical\n",
    "# from tensorflow import argmax, minimum, shape\n",
    "\n",
    "# # --- Your existing function imports ---\n",
    "# from src.nn_blocks import (\n",
    "#     unet_se_cnn,\n",
    "#     features_processing, \n",
    "#     GatedMixupGenerator, \n",
    "#     tof_block, \n",
    "#     match_time_steps, \n",
    "#     time_sum, \n",
    "#     squeeze_last_axis,\n",
    "#     expand_last_axis,\n",
    "#     crop_or_pad_output_shape\n",
    "# )\n",
    "\n",
    "# from src.functions import (\n",
    "#     train_model, \n",
    "#     create_sequence_dataset,\n",
    "#     perform_padding,\n",
    "#     generate_gate_targets\n",
    "# )\n",
    "# from src.constants import DATA_PATH\n",
    "# from src.tof_feats import remove_gravity_from_acc, calculate_angular_velocity_from_quat, calculate_angular_distance\n",
    "\n",
    "# def crop_or_pad(inputs):\n",
    "#     x, skip = inputs\n",
    "#     x_len = shape(x)[1]\n",
    "#     skip_len = shape(skip)[1]\n",
    "#     min_len = minimum(x_len, skip_len)\n",
    "#     return x[:, :min_len, :], skip[:, :min_len, :]\n",
    "\n",
    "# # =====================================================================================\n",
    "# # MASTER CONTROL FLAG\n",
    "# # =====================================================================================\n",
    "# TRAIN = True \n",
    "# TRAIN = False\n",
    "\n",
    "# # =====================================================================================\n",
    "# # CONFIGURATION\n",
    "# # =====================================================================================\n",
    "# PARQUET_FILE = 'output/final_processed_train_data.parquet'\n",
    "# PRETRAINED_DIR = Path(\"output/artifacts\")\n",
    "# PRETRAINED_DIR.mkdir(parents=True, exist_ok=True) # Ensure directory exists\n",
    "\n",
    "# LR_INIT = 5e-4\n",
    "# WD = 3e-3\n",
    "# NUM_CLASSES = 18\n",
    "# BATCH_SIZE = 64\n",
    "# N_SPLITS = 4 \n",
    "# MAX_PAD_LEN = 128\n",
    "\n",
    "# # --- 2. Define TTA Parameters and Predict Function ---\n",
    "# TTA_STEPS = 10\n",
    "# TTA_NOISE_STDDEV = 0.01\n",
    "\n",
    "# # =====================================================================================\n",
    "# # MODEL DEFINITION (Your existing function)\n",
    "# # =====================================================================================\n",
    "# def create_model(dataset, imu_dim, wd=1e-4):\n",
    "#     sample_batch = next(iter(dataset))\n",
    "#     input_shape = sample_batch[0].shape[1:]\n",
    "#     inp = tf.keras.layers.Input(shape=input_shape)\n",
    "#     imu = tf.keras.layers.Lambda(lambda t: t[:, :, :imu_dim])(inp)\n",
    "#     tof = tf.keras.layers.Lambda(lambda t: t[:, :, imu_dim:])(inp)\n",
    "\n",
    "#     x1 = unet_se_cnn(imu, 3, base_filters=64, kernel_size=3)\n",
    "#     x2 = tof_block(tof, wd)\n",
    "\n",
    "#     x = features_processing(x1, x2)\n",
    "#     x = tf.keras.layers.Dropout(0.3)(x) \n",
    "#     main_out = tf.keras.layers.Dense(18, activation=\"softmax\", name=\"main_output\")(x)\n",
    "#     gate_out = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"tof_gate\")(x) # Renamed layer\n",
    "    \n",
    "#     return tf.keras.models.Model(inputs=inp, outputs={\"main_output\": main_out, \"tof_gate\": gate_out})\n",
    "\n",
    "# # --- 1. Load All Inference Artifacts ---\n",
    "# print(\"▶ LOCAL DEBUG MODE – loading artefacts from\", PRETRAINED_DIR)\n",
    "# try:\n",
    "#     final_feature_cols = np.load(PRETRAINED_DIR / \"feature_cols.npy\", allow_pickle=True).tolist()\n",
    "#     pad_len = int(np.load(PRETRAINED_DIR / \"sequence_maxlen.npy\"))\n",
    "#     scaler = joblib.load(PRETRAINED_DIR / \"scaler.pkl\")\n",
    "#     gesture_classes = np.load(PRETRAINED_DIR / \"gesture_classes.npy\", allow_pickle=True)\n",
    "\n",
    "#     models = []\n",
    "#     print(f\"  Loading {N_SPLITS} models for ensemble inference...\")\n",
    "#     for fold in range(N_SPLITS):\n",
    "#         model_path = PRETRAINED_DIR / f\"gesture_model_fold_{fold}.h5\"\n",
    "#         model = load_model(model_path, compile=False, custom_objects={\n",
    "#             'unet_se_cnn': unet_se_cnn,\n",
    "#             'tof_block': tof_block,\n",
    "#             'features_processing': features_processing,\n",
    "#             'match_time_steps': match_time_steps,\n",
    "#             'crop_or_pad': crop_or_pad,\n",
    "#             'squeeze_last_axis': squeeze_last_axis,\n",
    "#             'expand_last_axis': expand_last_axis,\n",
    "#             'time_sum': time_sum,\n",
    "#             'crop_or_pad_output_shape': crop_or_pad_output_shape\n",
    "#         })\n",
    "#         models.append(model)\n",
    "#     print(\"  Models, scaler, and metadata loaded.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"ERROR loading artifacts: {e}\")\n",
    "#     # Stop execution if artifacts can't be loaded\n",
    "#     exit()\n",
    "\n",
    "# # --- 2. Define the Predict Function (Using the most robust version) ---\n",
    "# def predict(sequence: pl.DataFrame, demographics: pl.DataFrame) -> str:\n",
    "#     # ... (All your feature engineering code is correct and can remain the same) ...\n",
    "#     df_seq = sequence.to_pandas()\n",
    "#     # ... (Sanitization, feature creation, scaling, padding) ...\n",
    "#     sensor_cols = [c for c in df_seq.columns if c.startswith(('acc_', 'rot_', 'thm_', 'tof_'))]\n",
    "#     for col in sensor_cols:\n",
    "#         if df_seq[col].dtype == 'object':\n",
    "#             df_seq[col] = pd.to_numeric(df_seq[col], errors='coerce')\n",
    "#     new_features = {}\n",
    "#     linear_accel = remove_gravity_from_acc(df_seq, df_seq)\n",
    "#     new_features['linear_acc_x'] = linear_accel[:, 0]\n",
    "#     new_features['linear_acc_y'] = linear_accel[:, 1]\n",
    "#     new_features['linear_acc_z'] = linear_accel[:, 2]\n",
    "#     linear_acc_mag = np.sqrt(np.square(linear_accel).sum(axis=1))\n",
    "#     new_features['linear_acc_mag'] = linear_acc_mag\n",
    "#     new_features['linear_acc_mag_jerk'] = pd.Series(linear_acc_mag).diff().fillna(0).values\n",
    "#     angular_vel = calculate_angular_velocity_from_quat(df_seq)\n",
    "#     new_features['angular_vel_x'] = angular_vel[:, 0]\n",
    "#     new_features['angular_vel_y'] = angular_vel[:, 1]\n",
    "#     new_features['angular_vel_z'] = angular_vel[:, 2]\n",
    "#     new_features['angular_distance'] = calculate_angular_distance(df_seq)\n",
    "#     for i in range(1, 6):\n",
    "#         pixel_cols = [f\"tof_{i}_v{p}\" for p in range(64)]\n",
    "#         tof_data = df_seq[pixel_cols].replace(-1, np.nan)\n",
    "#         new_features[f'tof_{i}_mean'] = tof_data.mean(axis=1)\n",
    "#         new_features[f'tof_{i}_std'] = tof_data.std(axis=1)\n",
    "#         new_features[f'tof_{i}_min'] = tof_data.min(axis=1)\n",
    "#         new_features[f'tof_{i}_max'] = tof_data.max(axis=1)\n",
    "#     df_seq = df_seq.assign(**new_features)\n",
    "#     mat_unscaled_df = df_seq[final_feature_cols].ffill().bfill().fillna(0)\n",
    "#     mat_scaled = scaler.transform(mat_unscaled_df)\n",
    "#     pad_input = pad_sequences([mat_scaled], maxlen=pad_len, padding='post', truncating='post', dtype='float32')\n",
    "\n",
    "#     # --- TTA Loop ---\n",
    "#     all_tta_predictions = []\n",
    "#     for i in range(TTA_STEPS):\n",
    "#         noisy_input = pad_input\n",
    "#         if i > 0:\n",
    "#             noise = tf.random.normal(shape=tf.shape(pad_input), mean=0.0, stddev=TTA_NOISE_STDDEV)\n",
    "#             noisy_input = pad_input + noise\n",
    "\n",
    "#         # Ensemble predictions from all fold models\n",
    "#         all_fold_predictions = []\n",
    "#         for model in models:\n",
    "            \n",
    "#             # =========================================================================\n",
    "#             # --- THE FINAL FIX IS HERE ---\n",
    "#             # =========================================================================\n",
    "#             # model.predict returns a dictionary, access the 'main_output' key\n",
    "#             predictions_dict = model.predict(noisy_input, verbose=0)\n",
    "#             main_preds = predictions_dict['main_output']\n",
    "            \n",
    "#             all_fold_predictions.append(main_preds)\n",
    "        \n",
    "#         avg_fold_prediction = np.mean(all_fold_predictions, axis=0)\n",
    "#         all_tta_predictions.append(avg_fold_prediction)\n",
    "\n",
    "#     # --- Final Averaging and Prediction (Unchanged) ---\n",
    "#     final_avg_prediction = np.mean(all_tta_predictions, axis=0)\n",
    "#     idx = int(final_avg_prediction.argmax())\n",
    "    \n",
    "#     return str(gesture_classes[idx])\n",
    "\n",
    "# # =====================================================================================\n",
    "# # --- LOCAL TEST HARNESS ---\n",
    "# # =====================================================================================\n",
    "# print(\"\\n--- Starting Local Test ---\")\n",
    "\n",
    "# # Load the actual test data\n",
    "# TEST_CSV_PATH = 'input/cmi-detect-behavior-with-sensor-data/test.csv'\n",
    "# TEST_DEM_PATH = 'input/cmi-detect-behavior-with-sensor-data/test_demographics.csv'\n",
    "\n",
    "# try:\n",
    "#     test_df = pl.read_csv(TEST_CSV_PATH)\n",
    "#     test_dem_df = pl.read_csv(TEST_DEM_PATH)\n",
    "    \n",
    "#     # Pick the first sequence from the test set\n",
    "#     target_sequence_id = test_df.get_column(\"sequence_id\").unique()[0]\n",
    "#     print(f\"Testing with sequence_id: {target_sequence_id}\")\n",
    "    \n",
    "#     # Isolate the data for that single sequence\n",
    "#     sample_sequence_pl = test_df.filter(pl.col(\"sequence_id\") == target_sequence_id)\n",
    "    \n",
    "#     # Find the corresponding subject and their demographics\n",
    "#     subject_id = sample_sequence_pl.get_column(\"subject\")[0]\n",
    "#     sample_demographics_pl = test_dem_df.filter(pl.col(\"subject\") == subject_id)\n",
    "    \n",
    "#     # --- Call the predict function directly and catch the REAL error ---\n",
    "#     print(\"\\nCalling predict function directly...\")\n",
    "#     predicted_gesture = predict(sample_sequence_pl, sample_demographics_pl)\n",
    "    \n",
    "#     print(\"\\n✅ SUCCESS! The function ran without errors on a sample.\")\n",
    "#     print(f\"Predicted Gesture: {predicted_gesture}\")\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(\"\\n❌ ERROR! The function failed. Here is the full Python traceback:\")\n",
    "#     traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "007b5561",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_fold_predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mall_fold_predictions\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'all_fold_predictions' is not defined"
     ]
    }
   ],
   "source": [
    "all_fold_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d55d01fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 8)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>subject</th><th>adult_child</th><th>age</th><th>sex</th><th>handedness</th><th>height_cm</th><th>shoulder_to_wrist_cm</th><th>elbow_to_wrist_cm</th></tr><tr><td>str</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>f64</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;SUBJ_055840&quot;</td><td>0</td><td>13</td><td>0</td><td>1</td><td>177.0</td><td>52</td><td>27.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 8)\n",
       "┌─────────────┬─────────────┬─────┬─────┬────────────┬───────────┬────────────────┬────────────────┐\n",
       "│ subject     ┆ adult_child ┆ age ┆ sex ┆ handedness ┆ height_cm ┆ shoulder_to_wr ┆ elbow_to_wrist │\n",
       "│ ---         ┆ ---         ┆ --- ┆ --- ┆ ---        ┆ ---       ┆ ist_cm         ┆ _cm            │\n",
       "│ str         ┆ i64         ┆ i64 ┆ i64 ┆ i64        ┆ f64       ┆ ---            ┆ ---            │\n",
       "│             ┆             ┆     ┆     ┆            ┆           ┆ i64            ┆ f64            │\n",
       "╞═════════════╪═════════════╪═════╪═════╪════════════╪═══════════╪════════════════╪════════════════╡\n",
       "│ SUBJ_055840 ┆ 0           ┆ 13  ┆ 0   ┆ 1          ┆ 177.0     ┆ 52             ┆ 27.0           │\n",
       "└─────────────┴─────────────┴─────┴─────┴────────────┴───────────┴────────────────┴────────────────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_demographics_pl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
