{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea175bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-10 00:20:40.601671: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754781640.621021    6477 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754781640.626572    6477 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1754781640.642059    6477 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754781640.642085    6477 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754781640.642087    6477 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754781640.642089    6477 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-10 00:20:40.647269: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ INFERENCE MODE – loading artefacts from output/artifacts\n",
      "  Loading 4 models for ensemble inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1754781643.947011    6477 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4714 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1060, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Models, scaler, and metadata loaded – ready for evaluation.\n",
      "Running local gateway for testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uallende/projects/kaggle/CMI/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "I0000 00:00:1754781650.838803    6586 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "ename": "GatewayRuntimeError",
     "evalue": "(<GatewayRuntimeErrorType.SERVER_RAISED_EXCEPTION: 3>, \"the resolved dtypes are not compatible with add.reduce. Resolved (dtype('<U11'), dtype('<U11'), dtype('<U22'))\")",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mGatewayRuntimeError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 300\u001b[39m\n\u001b[32m    297\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    298\u001b[39m     \u001b[38;5;66;03m# For local testing, you need to provide the paths to the test data\u001b[39;00m\n\u001b[32m    299\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRunning local gateway for testing...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m     \u001b[43minference_server\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_local_gateway\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata_paths\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minput/cmi-detect-behavior-with-sensor-data/test.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minput/cmi-detect-behavior-with-sensor-data/test_demographics.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/kaggle/CMI/kaggle_evaluation/core/templates.py:149\u001b[39m, in \u001b[36mInferenceServer.run_local_gateway\u001b[39m\u001b[34m(self, data_paths, file_share_dir, *args, **kwargs)\u001b[39m\n\u001b[32m    147\u001b[39m     \u001b[38;5;28mself\u001b[39m.gateway.run()\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m err \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    151\u001b[39m     \u001b[38;5;28mself\u001b[39m.server.stop(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/kaggle/CMI/kaggle_evaluation/core/templates.py:147\u001b[39m, in \u001b[36mInferenceServer.run_local_gateway\u001b[39m\u001b[34m(self, data_paths, file_share_dir, *args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    146\u001b[39m     \u001b[38;5;28mself\u001b[39m.gateway = \u001b[38;5;28mself\u001b[39m._get_gateway_for_test(data_paths, file_share_dir, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    149\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m err \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/kaggle/CMI/kaggle_evaluation/core/templates.py:105\u001b[39m, in \u001b[36mGateway.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    102\u001b[39m     \u001b[38;5;28mself\u001b[39m.write_result(error)\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m error:\n\u001b[32m    104\u001b[39m     \u001b[38;5;66;03m# For local testing\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/kaggle/CMI/kaggle_evaluation/core/templates.py:84\u001b[39m, in \u001b[36mGateway.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     83\u001b[39m     \u001b[38;5;28mself\u001b[39m.unpack_data_paths()\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     predictions, row_ids = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_all_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m     \u001b[38;5;28mself\u001b[39m.write_submission(predictions, row_ids)\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m kaggle_evaluation.core.base_gateway.GatewayRuntimeError \u001b[38;5;28;01mas\u001b[39;00m gre:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/kaggle/CMI/kaggle_evaluation/core/templates.py:56\u001b[39m, in \u001b[36mGateway.get_all_predictions\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     54\u001b[39m all_row_ids = []\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m data_batch, row_ids \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.generate_data_batches():\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     predictions = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdata_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m     \u001b[38;5;28mself\u001b[39m.validate_prediction_batch(predictions, row_ids)\n\u001b[32m     58\u001b[39m     all_predictions.append(predictions)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/kaggle/CMI/kaggle_evaluation/core/templates.py:72\u001b[39m, in \u001b[36mGateway.predict\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.client.send(\u001b[33m'\u001b[39m\u001b[33mpredict\u001b[39m\u001b[33m'\u001b[39m, *args, **kwargs)\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle_server_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpredict\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/kaggle/CMI/kaggle_evaluation/core/base_gateway.py:257\u001b[39m, in \u001b[36mBaseGateway.handle_server_error\u001b[39m\u001b[34m(self, exception, endpoint)\u001b[39m\n\u001b[32m    255\u001b[39m     message_match = re.search(\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mException calling application: (.*)\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m, exception_str, re.IGNORECASE)\n\u001b[32m    256\u001b[39m     message = message_match.group(\u001b[32m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m message_match \u001b[38;5;28;01melse\u001b[39;00m exception_str\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m GatewayRuntimeError(GatewayRuntimeErrorType.SERVER_RAISED_EXCEPTION, message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exception, grpc._channel._InactiveRpcError):\n\u001b[32m    259\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m GatewayRuntimeError(GatewayRuntimeErrorType.SERVER_CONNECTION_FAILED, exception_str) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mGatewayRuntimeError\u001b[39m: (<GatewayRuntimeErrorType.SERVER_RAISED_EXCEPTION: 3>, \"the resolved dtypes are not compatible with add.reduce. Resolved (dtype('<U11'), dtype('<U11'), dtype('<U22'))\")"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import joblib\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import pad_sequences, to_categorical\n",
    "\n",
    "# --- Your existing function imports ---\n",
    "from src.nn_blocks import (\n",
    "    unet_se_cnn,\n",
    "    features_processing, \n",
    "    GatedMixupGenerator, \n",
    "    tof_block, \n",
    "    match_time_steps, \n",
    "    time_sum, \n",
    "    squeeze_last_axis,\n",
    "    expand_last_axis\n",
    ")\n",
    "\n",
    "from src.functions import (\n",
    "    train_model, \n",
    "    create_sequence_dataset,\n",
    "    perform_padding,\n",
    "    generate_gate_targets\n",
    ")\n",
    "from src.constants import DATA_PATH\n",
    "\n",
    "# =====================================================================================\n",
    "# MASTER CONTROL FLAG\n",
    "# =====================================================================================\n",
    "TRAIN = True \n",
    "TRAIN = False\n",
    "\n",
    "# =====================================================================================\n",
    "# CONFIGURATION\n",
    "# =====================================================================================\n",
    "PARQUET_FILE = 'output/final_processed_train_data.parquet'\n",
    "PRETRAINED_DIR = Path(\"output/artifacts\")\n",
    "PRETRAINED_DIR.mkdir(parents=True, exist_ok=True) # Ensure directory exists\n",
    "\n",
    "LR_INIT = 5e-4\n",
    "WD = 3e-3\n",
    "NUM_CLASSES = 18\n",
    "BATCH_SIZE = 64\n",
    "N_SPLITS = 4 \n",
    "MAX_PAD_LEN = 128\n",
    "\n",
    "# =====================================================================================\n",
    "# MODEL DEFINITION (Your existing function)\n",
    "# =====================================================================================\n",
    "def create_model(dataset, imu_dim, wd=1e-4):\n",
    "    sample_batch = next(iter(dataset))\n",
    "    input_shape = sample_batch[0].shape[1:]\n",
    "    inp = tf.keras.layers.Input(shape=input_shape)\n",
    "    imu = tf.keras.layers.Lambda(lambda t: t[:, :, :imu_dim])(inp)\n",
    "    tof = tf.keras.layers.Lambda(lambda t: t[:, :, imu_dim:])(inp)\n",
    "\n",
    "    x1 = unet_se_cnn(imu, 3, base_filters=64, kernel_size=3)\n",
    "    x2 = tof_block(tof, wd)\n",
    "\n",
    "    x = features_processing(x1, x2)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x) \n",
    "    main_out = tf.keras.layers.Dense(18, activation=\"softmax\", name=\"main_output\")(x)\n",
    "    gate_out = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"tof_gate\")(x) # Renamed layer\n",
    "    \n",
    "    return tf.keras.models.Model(inputs=inp, outputs={\"main_output\": main_out, \"tof_gate\": gate_out})\n",
    "\n",
    "# =====================================================================================\n",
    "# TRAINING LOGIC\n",
    "# =====================================================================================\n",
    "if TRAIN:\n",
    "    schema_df = pl.read_parquet(PARQUET_FILE, n_rows=0)\n",
    "    all_columns = schema_df.columns\n",
    "    meta_cols = {'gesture', 'gesture_int', 'sequence_type', 'behavior', 'orientation',\n",
    "                    'row_id', 'subject', 'phase', 'sequence_id', 'sequence_counter'}\n",
    "    feature_cols = [c for c in all_columns if c not in meta_cols]\n",
    "    imu_cols  = [c for c in feature_cols if not (c.startswith('thm_') or c.startswith('tof_'))]\n",
    "    tof_cols  = [c for c in feature_cols if c.startswith('thm_') or c.startswith('tof_')]\n",
    "\n",
    "    print(\"Scanning Parquet file for sequence IDs...\")\n",
    "    all_sequence_ids = (\n",
    "        pl.scan_parquet(PARQUET_FILE)\n",
    "        .select('sequence_id')\n",
    "        .unique()\n",
    "        .collect()\n",
    "        .to_numpy()\n",
    "        .ravel()\n",
    "    )\n",
    "    print(f\"Found {len(all_sequence_ids)} unique sequences.\")\n",
    "\n",
    "    kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "    fold_accuracies = []\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    imu_dim = len(imu_cols)\n",
    "\n",
    "    for fold_idx, (train_indices, val_indices) in enumerate(kf.split(all_sequence_ids)):\n",
    "        print(f\"\\n=== Fold {fold_idx + 1}/{N_SPLITS} ===\")\n",
    "        train_ids = all_sequence_ids[train_indices]\n",
    "        val_ids = all_sequence_ids[val_indices]\n",
    "\n",
    "        print(f\"Loading data for fold {fold_idx + 1}...\")\n",
    "        train_df = pl.read_parquet(PARQUET_FILE).filter(pl.col('sequence_id').is_in(train_ids))\n",
    "        val_df = pl.read_parquet(PARQUET_FILE).filter(pl.col('sequence_id').is_in(val_ids))\n",
    "        print(\"Fold data loaded.\")\n",
    "\n",
    "        train_gate_df = generate_gate_targets(train_df, tof_cols)\n",
    "        val_gate_df = generate_gate_targets(val_df, tof_cols)\n",
    "\n",
    "        le = LabelEncoder()\n",
    "        le.fit(train_df['gesture'])\n",
    "        train_df = train_df.with_columns(pl.Series(\"gesture_int\", le.transform(train_df['gesture'])))\n",
    "        val_df = val_df.with_columns(pl.Series(\"gesture_int\", le.transform(val_df['gesture'])))\n",
    "\n",
    "        # --- StandardScaler Logic ---\n",
    "        scaler = StandardScaler()\n",
    "        # Fit on training data and transform both\n",
    "        train_features_scaled = scaler.fit_transform(train_df[imu_cols + tof_cols])\n",
    "        val_features_scaled = scaler.transform(val_df[imu_cols + tof_cols])\n",
    "        # Create Polars DataFrames from the scaled numpy arrays\n",
    "        X_train_scaled_features = pl.DataFrame(train_features_scaled, schema=imu_cols + tof_cols)\n",
    "        X_val_scaled_features = pl.DataFrame(val_features_scaled, schema=imu_cols + tof_cols)\n",
    "\n",
    "        meta_cols_to_keep = ['sequence_id', 'gesture_int']\n",
    "        train_df_final = train_df.select(meta_cols_to_keep).with_columns(X_train_scaled_features)\n",
    "        val_df_final = val_df.select(meta_cols_to_keep).with_columns(X_val_scaled_features)\n",
    "\n",
    "        del train_df, val_df, X_train_scaled_features, X_val_scaled_features\n",
    "        gc.collect()\n",
    "\n",
    "        X_train, y_train, train_gate_target = create_sequence_dataset(train_df_final, imu_cols + tof_cols, train_gate_df)\n",
    "        X_val, y_val, val_gate_target = create_sequence_dataset(val_df_final, imu_cols + tof_cols, val_gate_df)\n",
    "\n",
    "        del train_df_final, val_df_final\n",
    "        gc.collect()\n",
    "\n",
    "        X_train_padded = perform_padding(X_train, MAX_PAD_LEN)\n",
    "        X_val_padded = perform_padding(X_val, MAX_PAD_LEN)\n",
    "        \n",
    "        y_train_cat = to_categorical(y_train, num_classes=NUM_CLASSES)\n",
    "        y_val_cat = to_categorical(y_val, num_classes=NUM_CLASSES)\n",
    "\n",
    "        train_dataset = GatedMixupGenerator(\n",
    "            X=X_train_padded, y=y_train_cat, gate_targets=train_gate_target,\n",
    "            batch_size=BATCH_SIZE, imu_dim=imu_dim, alpha=0.2, masking_prob=0.25\n",
    "        )\n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "            X_val_padded, {'main_output': y_val_cat, 'tof_gate': val_gate_target[:, np.newaxis]}\n",
    "        )).batch(BATCH_SIZE).cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        del X_val, y_val, X_train, y_train, X_train_padded, X_val_padded\n",
    "        gc.collect()\n",
    "        \n",
    "        model = create_model(train_dataset, len(imu_cols))\n",
    "        train_model(model, train_dataset, val_dataset, 150, LR_INIT, WD)\n",
    "\n",
    "        # --- SAVE ARTIFACTS ---\n",
    "        print(f\"--- Saving artifacts for Fold {fold_idx + 1} ---\")\n",
    "        model.save(PRETRAINED_DIR / f\"gesture_model_fold_{fold_idx}.h5\")\n",
    "        \n",
    "        # Save scaler and other metadata only from the first fold\n",
    "        if fold_idx == 0:\n",
    "            joblib.dump(scaler, PRETRAINED_DIR / \"scaler.pkl\")\n",
    "            np.save(PRETRAINED_DIR / \"feature_cols.npy\", np.array(imu_cols + tof_cols))\n",
    "            np.save(PRETRAINED_DIR / \"sequence_maxlen.npy\", MAX_PAD_LEN)\n",
    "            np.save(PRETRAINED_DIR / \"gesture_classes.npy\", le.classes_)\n",
    "            print(\"Scaler, feature_cols, maxlen, and classes saved.\")\n",
    "\n",
    "        # --- EVALUATION ---\n",
    "        val_preds = model.predict(val_dataset)\n",
    "        main_output_preds = val_preds['main_output']\n",
    "        y_pred_fold = np.argmax(main_output_preds, axis=1)\n",
    "        y_true_fold = np.argmax(y_val_cat, axis=1)\n",
    "        fold_acc = accuracy_score(y_true_fold, y_pred_fold)\n",
    "        fold_accuracies.append(fold_acc)\n",
    "        print(f\"Fold {fold_idx + 1} Accuracy: {fold_acc:.4f}\")\n",
    "        all_preds.append(y_pred_fold)\n",
    "        all_labels.append(y_true_fold)\n",
    "\n",
    "        del train_dataset, model, val_dataset\n",
    "        gc.collect()\n",
    "\n",
    "    # --- FINAL OOF REPORT ---\n",
    "    print(\"\\n=== Cross-validation Summary ===\")\n",
    "    print(f\"Per-fold Accuracies: {fold_accuracies}\")\n",
    "    print(f\"Mean Accuracy: {np.mean(fold_accuracies):.4f} ± {np.std(fold_accuracies):.4f}\")\n",
    "    y_all_pred = np.concatenate(all_preds)\n",
    "    y_all_true = np.concatenate(all_labels)\n",
    "    print(\"\\n=== Overall Classification Report ===\")\n",
    "    print(classification_report(y_all_true, y_all_pred, target_names=le.classes_, digits=4))\n",
    "\n",
    "# =====================================================================================\n",
    "# INFERENCE LOGIC\n",
    "# =====================================================================================\n",
    "else:\n",
    "    import pandas as pd \n",
    "    from src.metric import CompetitionMetric \n",
    "    from tensorflow import argmax, minimum, shape\n",
    "\n",
    "    def crop_or_pad(inputs):\n",
    "        x, skip = inputs\n",
    "        x_len = shape(x)[1]\n",
    "        skip_len = shape(skip)[1]\n",
    "        min_len = minimum(x_len, skip_len)\n",
    "        return x[:, :min_len, :], skip[:, :min_len, :]\n",
    "    \n",
    "    # --- 1. Load All Inference Artifacts ---\n",
    "    print(\"▶ INFERENCE MODE – loading artefacts from\", PRETRAINED_DIR)\n",
    "    final_feature_cols = np.load(PRETRAINED_DIR / \"feature_cols.npy\", allow_pickle=True).tolist()\n",
    "    pad_len = int(np.load(PRETRAINED_DIR / \"sequence_maxlen.npy\"))\n",
    "    scaler = joblib.load(PRETRAINED_DIR / \"scaler.pkl\")\n",
    "    gesture_classes = np.load(PRETRAINED_DIR / \"gesture_classes.npy\", allow_pickle=True)\n",
    "\n",
    "    models = []\n",
    "    print(f\"  Loading {N_SPLITS} models for ensemble inference...\")\n",
    "    for fold in range(N_SPLITS):\n",
    "        model_path = PRETRAINED_DIR / f\"gesture_model_fold_{fold}.h5\"\n",
    "        # Note: You only need to pass custom LAYERS/FUNCTIONS here, not the whole model definition\n",
    "        model = load_model(model_path, compile=False, custom_objects={\n",
    "            'unet_se_cnn': unet_se_cnn,\n",
    "            'tof_block': tof_block,\n",
    "            'features_processing': features_processing,\n",
    "            'match_time_steps': match_time_steps,\n",
    "            'crop_or_pad': crop_or_pad,\n",
    "            'squeeze_last_axis': squeeze_last_axis,\n",
    "            'expand_last_axis': expand_last_axis,\n",
    "            'time_sum': time_sum\n",
    "        })\n",
    "        models.append(model)\n",
    "    print(\"  Models, scaler, and metadata loaded – ready for evaluation.\")\n",
    "\n",
    "    # --- 2. Define TTA Parameters and Predict Function ---\n",
    "    TTA_STEPS = 10\n",
    "    TTA_NOISE_STDDEV = 0.01\n",
    "\n",
    "    from src.tof_feats import remove_gravity_from_acc, calculate_angular_velocity_from_quat, calculate_angular_distance\n",
    "\n",
    "    def predict(sequence: pl.DataFrame, demographics: pl.DataFrame) -> str:\n",
    "        df_seq = sequence.to_pandas()\n",
    "        \n",
    "        # --- Feature Engineering (must match training) ---\n",
    "        linear_accel = remove_gravity_from_acc(df_seq, df_seq)\n",
    "        df_seq['linear_acc_x'], df_seq['linear_acc_y'], df_seq['linear_acc_z'] = linear_accel[:, 0], linear_accel[:, 1], linear_accel[:, 2]\n",
    "        df_seq['linear_acc_mag'] = np.sqrt(df_seq['linear_acc_x']**2 + df_seq['linear_acc_y']**2 + df_seq['linear_acc_z']**2)\n",
    "        df_seq['linear_acc_mag_jerk'] = df_seq['linear_acc_mag'].diff().fillna(0)\n",
    "        angular_vel = calculate_angular_velocity_from_quat(df_seq)\n",
    "        df_seq['angular_vel_x'], df_seq['angular_vel_y'], df_seq['angular_vel_z'] = angular_vel[:, 0], angular_vel[:, 1], angular_vel[:, 2]\n",
    "        df_seq['angular_distance'] = calculate_angular_distance(df_seq)\n",
    "        for i in range(1, 6):\n",
    "            pixel_cols = [f\"tof_{i}_v{p}\" for p in range(64)]\n",
    "            tof_data = df_seq[pixel_cols].replace(-1, np.nan)\n",
    "            df_seq[f'tof_{i}_mean'] = tof_data.mean(axis=1)\n",
    "            df_seq[f'tof_{i}_std'] = tof_data.std(axis=1)\n",
    "            df_seq[f'tof_{i}_min'] = tof_data.min(axis=1)\n",
    "            df_seq[f'tof_{i}_max'] = tof_data.max(axis=1)\n",
    "            \n",
    "        mat_unscaled = df_seq[final_feature_cols].ffill().bfill().fillna(0).values.astype('float32')\n",
    "        mat_scaled = scaler.transform(mat_unscaled)\n",
    "        pad_input = pad_sequences([mat_scaled], maxlen=pad_len, padding='post', truncating='post', dtype='float32')\n",
    "\n",
    "        # --- TTA Loop ---\n",
    "        all_tta_predictions = []\n",
    "        for i in range(TTA_STEPS):\n",
    "            noisy_input = pad_input\n",
    "            if i > 0: # First pass is always on clean data\n",
    "                noise = tf.random.normal(shape=tf.shape(pad_input), mean=0.0, stddev=TTA_NOISE_STDDEV)\n",
    "                noisy_input = pad_input + noise\n",
    "\n",
    "            # Ensemble predictions from all fold models\n",
    "            all_fold_predictions = []\n",
    "            for model in models:\n",
    "                main_preds, _ = model.predict(noisy_input, verbose=0)\n",
    "                all_fold_predictions.append(main_preds)\n",
    "            \n",
    "            avg_fold_prediction = np.mean(all_fold_predictions, axis=0)\n",
    "            all_tta_predictions.append(avg_fold_prediction)\n",
    "\n",
    "        # --- Final Averaging and Prediction ---\n",
    "        final_avg_prediction = np.mean(all_tta_predictions, axis=0)\n",
    "        idx = int(final_avg_prediction.argmax())\n",
    "        \n",
    "        return str(gesture_classes[idx])\n",
    "\n",
    "    # --- 3. Run Kaggle Evaluation Server ---\n",
    "    import kaggle_evaluation.cmi_inference_server\n",
    "    inference_server = kaggle_evaluation.cmi_inference_server.CMIInferenceServer(predict)\n",
    "\n",
    "    if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "        inference_server.serve()\n",
    "    else:\n",
    "        # For local testing, you need to provide the paths to the test data\n",
    "        print(\"Running local gateway for testing...\")\n",
    "        inference_server.run_local_gateway(\n",
    "            data_paths=(\n",
    "                'input/cmi-detect-behavior-with-sensor-data/test.csv',\n",
    "                'input/cmi-detect-behavior-with-sensor-data/test_demographics.csv',\n",
    "            )\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
