{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea175bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 09:48:12.532117: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754988492.729943  871620 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754988492.788142  871620 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1754988493.295576  871620 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754988493.295619  871620 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754988493.295621  871620 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754988493.295623  871620 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-12 09:48:13.333851: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building base DataFrame with metadata...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import joblib\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import pad_sequences, to_categorical\n",
    "\n",
    "from src.nn_blocks import (\n",
    "    unet_se_cnn,\n",
    "    features_processing, \n",
    "    GatedMixupGenerator, \n",
    "    tof_block, \n",
    "    match_time_steps, \n",
    "    time_sum, \n",
    "    squeeze_last_axis,\n",
    "    expand_last_axis,\n",
    "    crop_or_pad_output_shape\n",
    ")\n",
    "\n",
    "from src.functions import (\n",
    "    train_model, \n",
    "    create_sequence_dataset,\n",
    "    perform_padding,\n",
    "    generate_gate_targets\n",
    ")\n",
    "\n",
    "from src.merge_feats_dynamic import merge_feature_sets\n",
    "\n",
    "# =====================================================================================\n",
    "# MASTER CONTROL FLAG\n",
    "# =====================================================================================\n",
    "TRAIN = False\n",
    "TRAIN = True \n",
    "\n",
    "# =====================================================================================\n",
    "# CONFIGURATION\n",
    "# =====================================================================================\n",
    "PARQUET_FILE = 'output/final_model_input_dataset.parquet'\n",
    "# PARQUET_FILE = \"data/extended_features_df.parquet\"\n",
    "# PARQUET_FILE = 'output/kaggle_0.8_feats.parquet'\n",
    "PRETRAINED_DIR = Path(\"output/artifacts\")\n",
    "PRETRAINED_DIR.mkdir(parents=True, exist_ok=True) # Ensure directory exists\n",
    "\n",
    "LR_INIT = 5e-4\n",
    "WD = 3e-3\n",
    "NUM_CLASSES = 18\n",
    "BATCH_SIZE = 64\n",
    "N_SPLITS = 4 \n",
    "MAX_PAD_LEN = 128\n",
    "\n",
    "def create_sequence_dataset_simple(df: pl.DataFrame, feature_cols: list):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for seq_id, group in df.group_by('sequence_id', maintain_order=True):\n",
    "        sequences.append(group.select(feature_cols).to_numpy())\n",
    "        labels.append(group.select('gesture_int').item(0, 0))\n",
    "    return np.array(sequences, dtype=object), np.array(labels)\n",
    "\n",
    "\n",
    "\n",
    "# =====================================================================================\n",
    "# MODEL DEFINITION (Your existing function)\n",
    "# =====================================================================================\n",
    "\n",
    "from src.nn_blocks import (\n",
    "    wave_block, residual_se_cnn_block, tof_block_2, attention_layer\n",
    ")\n",
    "\n",
    "def create_model(dataset, imu_dim, wd=1e-4):\n",
    "    sample_batch = next(iter(dataset))\n",
    "    input_shape = sample_batch[0].shape[1:]\n",
    "    inp = tf.keras.layers.Input(shape=input_shape)\n",
    "    imu = tf.keras.layers.Lambda(lambda t: t[:, :, :imu_dim])(inp)\n",
    "\n",
    "    x = unet_se_cnn(imu, 3, base_filters=128, kernel_size=3)\n",
    "    x = attention_layer(x) # Assuming attention_layer is defined \n",
    "    x = tf.keras.layers.Dropout(0.3)(x) \n",
    "\n",
    "    main_out = tf.keras.layers.Dense(18, activation=\"softmax\", name=\"main_output\")(x)\n",
    "    return tf.keras.models.Model(inputs=inp, outputs=main_out)\n",
    "\n",
    "\n",
    "# =====================================================================================\n",
    "# TRAINING LOGIC\n",
    "# =====================================================================================\n",
    "\n",
    "\n",
    "FEATURE_DIR = Path('output')\n",
    "RAW_DIR = Path('input/cmi-detect-behavior-with-sensor-data')\n",
    "RANDOM_STATE = 42\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "if TRAIN:\n",
    "    # --- Step 1: Define the feature sets to merge for this experiment ---\n",
    "    files_to_merge = [\n",
    "        \"imu_physics_feats.parquet\",\n",
    "        # \"imu_rolling_stats_features.parquet\",\n",
    "        # \"imu_cross_modal_features.parquet\",\n",
    "        # ToF features are intentionally left out\n",
    "    ]\n",
    "    feature_paths = [FEATURE_DIR / f for f in files_to_merge]\n",
    "    \n",
    "    # --- Step 2: Build the base and final DataFrame ---\n",
    "    print(\"  Building base DataFrame with metadata...\")\n",
    "    base_df = pl.read_csv(RAW_DIR / \"train.csv\")\n",
    "    demographics_df = pl.read_csv(RAW_DIR / \"train_demographics.csv\")\n",
    "    base_df = base_df.join(demographics_df, on='subject', how='left')\n",
    "\n",
    "    # Perform Label Encoding and add 'gesture_int' BEFORE the merge.\n",
    "    print(\"  Performing label encoding...\")\n",
    "    le = LabelEncoder()\n",
    "    gesture_encoded = le.fit_transform(base_df.get_column('gesture'))\n",
    "    base_df = base_df.with_columns(pl.Series(\"gesture_int\", gesture_encoded))    \n",
    "    \n",
    "    # Use the modular merge function\n",
    "    final_df = merge_feature_sets(base_df, feature_paths)\n",
    "    print(f\"  Final merged DataFrame created with shape: {final_df.shape}\")\n",
    "\n",
    "    # --- Step 3: Define feature columns based on the *actual* merged data ---\n",
    "    all_columns = final_df.columns\n",
    "    meta_cols = {'gesture', 'gesture_int', 'sequence_type', 'behavior', 'orientation',\n",
    "                    'row_id', 'subject', 'phase', 'sequence_id', 'sequence_counter'}\n",
    "    # All non-metadata columns are now our IMU features\n",
    "    imu_cols = [c for c in all_columns if c not in meta_cols]\n",
    "    imu_dim = len(imu_cols)\n",
    "    print(f\"  Training with {imu_dim} IMU features.\")\n",
    "\n",
    "    # --- Step 4: Prepare for Cross-Validation ---\n",
    "    # Get sequence IDs and labels for stratified splitting\n",
    "    # This is more robust than scanning a different parquet file\n",
    "    cv_info = final_df.group_by(\"sequence_id\").agg(pl.first(\"gesture_int\")).sort(\"sequence_id\")\n",
    "    all_sequence_ids = cv_info.get_column(\"sequence_id\").to_numpy()\n",
    "    y_for_split = cv_info.get_column(\"gesture_int\").to_numpy()\n",
    "    \n",
    "    kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "    fold_accuracies = []\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for fold_idx, (train_indices, val_indices) in enumerate(kf.split(all_sequence_ids, y_for_split)):\n",
    "        print(f\"\\n=== Fold {fold_idx + 1}/{N_SPLITS} ===\")\n",
    "        train_ids = all_sequence_ids[train_indices]\n",
    "        val_ids = all_sequence_ids[val_indices]\n",
    "\n",
    "        # Filter the merged DataFrame for the current fold\n",
    "        train_df = final_df.filter(pl.col('sequence_id').is_in(train_ids))\n",
    "        val_df = final_df.filter(pl.col('sequence_id').is_in(val_ids))\n",
    "        print(\"Fold data loaded.\")\n",
    "\n",
    "        # Label encoding is already done, but we need the encoder for the final report\n",
    "        le = LabelEncoder().fit(train_df['gesture'])\n",
    "        \n",
    "        # --- StandardScaler Logic ---\n",
    "        scaler = StandardScaler()\n",
    "        train_features_scaled = scaler.fit_transform(train_df[imu_cols])\n",
    "        val_features_scaled = scaler.transform(val_df[imu_cols])\n",
    "        X_train_scaled_features = pl.DataFrame(train_features_scaled, schema=imu_cols)\n",
    "        X_val_scaled_features = pl.DataFrame(val_features_scaled, schema=imu_cols)\n",
    "\n",
    "        meta_cols_to_keep = ['sequence_id', 'gesture_int']\n",
    "        train_df_final = train_df.select(meta_cols_to_keep).with_columns(X_train_scaled_features)\n",
    "        val_df_final = val_df.select(meta_cols_to_keep).with_columns(X_val_scaled_features)\n",
    "\n",
    "        del train_df, val_df, X_train_scaled_features, X_val_scaled_features\n",
    "        gc.collect()\n",
    "\n",
    "        # Create sequences (no gate target needed)\n",
    "        X_train, y_train = create_sequence_dataset_simple(train_df_final, imu_cols)\n",
    "        X_val, y_val = create_sequence_dataset_simple(val_df_final, imu_cols)\n",
    "\n",
    "        del train_df_final, val_df_final\n",
    "        gc.collect()\n",
    "\n",
    "        X_train_padded = pad_sequences(X_train, maxlen=MAX_PAD_LEN, padding='post', truncating='post', dtype='float32')\n",
    "        X_val_padded = pad_sequences(X_val, maxlen=MAX_PAD_LEN, padding='post', truncating='post', dtype='float32')\n",
    "        \n",
    "        y_train_cat = to_categorical(y_train, num_classes=NUM_CLASSES)\n",
    "        y_val_cat = to_categorical(y_val, num_classes=NUM_CLASSES)\n",
    "\n",
    "        # Create simple TF Datasets (no generator needed unless you want mixup)\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((X_train_padded, y_train_cat)).shuffle(len(X_train_padded)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices((X_val_padded, y_val_cat)).batch(BATCH_SIZE).cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        del X_train, y_train, X_val, y_val, X_train_padded, X_val_padded\n",
    "        gc.collect()\n",
    "        \n",
    "        # Use the new IMU-only model\n",
    "        model = create_model(train_dataset)\n",
    "        \n",
    "        # Adapt the train_model call for a single output\n",
    "        optimizer = tf.keras.optimizers.AdamW(learning_rate=LR_INIT, weight_decay=WD)\n",
    "        model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "        early_stopping = tf.keras.optimizers.EarlyStopping(monitor='val_accuracy', patience=20, restore_best_weights=True, mode='max')\n",
    "        model.fit(train_dataset, validation_data=val_dataset, epochs=150, callbacks=[early_stopping])\n",
    "        \n",
    "        # --- EVALUATION ---\n",
    "        # The model now returns a single array, not a dictionary\n",
    "        y_pred_proba = model.predict(val_dataset)\n",
    "        y_pred_fold = np.argmax(y_pred_proba, axis=1)\n",
    "        y_true_fold = np.argmax(y_val_cat, axis=1)\n",
    "\n",
    "        # --- SAVE ARTIFACTS ---\n",
    "        print(f\"--- Saving artifacts for Fold {fold_idx + 1} ---\")\n",
    "        model.save(PRETRAINED_DIR / f\"gesture_model_fold_{fold_idx}.h5\")\n",
    "        \n",
    "        # # Save scaler and other metadata only from the first fold\n",
    "        # if fold_idx == 0:\n",
    "        #     joblib.dump(scaler, PRETRAINED_DIR / \"scaler.pkl\")\n",
    "        #     np.save(PRETRAINED_DIR / \"feature_cols.npy\", np.array(imu_cols))\n",
    "        #     np.save(PRETRAINED_DIR / \"sequence_maxlen.npy\", MAX_PAD_LEN)\n",
    "        #     np.save(PRETRAINED_DIR / \"gesture_classes.npy\", le.classes_)\n",
    "        #     print(\"Scaler, feature_cols, maxlen, and classes saved.\")\n",
    "\n",
    "        # --- EVALUATION ---\n",
    "        val_preds = model.predict(val_dataset)\n",
    "        main_output_preds = val_preds['main_output']\n",
    "        y_pred_fold = np.argmax(main_output_preds, axis=1)\n",
    "        y_true_fold = np.argmax(y_val_cat, axis=1)\n",
    "        fold_acc = accuracy_score(y_true_fold, y_pred_fold)\n",
    "        fold_accuracies.append(fold_acc)\n",
    "        print(f\"Fold {fold_idx + 1} Accuracy: {fold_acc:.4f}\")\n",
    "        all_preds.append(y_pred_fold)\n",
    "        all_labels.append(y_true_fold)\n",
    "\n",
    "        del train_dataset, model, val_dataset\n",
    "        gc.collect()\n",
    "\n",
    "    # --- FINAL OOF REPORT ---\n",
    "    print(\"\\n=== Cross-validation Summary ===\")\n",
    "    print(f\"Per-fold Accuracies: {fold_accuracies}\")\n",
    "    print(f\"Mean Accuracy: {np.mean(fold_accuracies):.4f} ± {np.std(fold_accuracies):.4f}\")\n",
    "    y_all_pred = np.concatenate(all_preds)\n",
    "    y_all_true = np.concatenate(all_labels)\n",
    "    print(\"\\n=== Overall Classification Report ===\")\n",
    "    print(classification_report(y_all_true, y_all_pred, target_names=le.classes_, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73f8589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "PARQUET_FILE = 'output/kaggle_0.8_feats.parquet'\n",
    "df = pl.read_parquet(PARQUET_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30587df",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrain_dataset\u001b[49m:\n\u001b[32m      2\u001b[39m     x = e[\u001b[32m0\u001b[39m]\n\u001b[32m      3\u001b[39m     y = e[\u001b[32m1\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "for e in train_dataset:\n",
    "    x = e[0]\n",
    "    y = e[1]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d3ad30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 128, 38)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f2ecb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 38)\n"
     ]
    }
   ],
   "source": [
    "input_shape = x[0].shape\n",
    "inp = tf.keras.layers.Input(shape=input_shape)\n",
    "imu = tf.keras.layers.Lambda(lambda t: t[:, :, :imu_dim])(inp)\n",
    "tof = tf.keras.layers.Lambda(lambda t: t[:, :, imu_dim:])(inp)\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7a87a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 32, 128)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wd = 0\n",
    "# TOF/Thermal lighter branch\n",
    "x2 = tf.keras.layers.Conv1D(64, 3, padding='same', use_bias=False, kernel_regularizer=tf.keras.regularizers.l2(wd))(tof)\n",
    "x2 = tf.keras.layers.BatchNormalization()(x2); x2 = tf.keras.layers.Activation('relu')(x2)\n",
    "x2 = tf.keras.layers.MaxPooling1D(2)(x2); x2 = tf.keras.layers.Dropout(0.2)(x2)\n",
    "x2 = tf.keras.layers.Conv1D(128, 3, padding='same', use_bias=False, kernel_regularizer=tf.keras.regularizers.l2(wd))(x2)\n",
    "x2 = tf.keras.layers.BatchNormalization()(x2); x2 = tf.keras.layers.Activation('relu')(x2)\n",
    "x2 = tf.keras.layers.MaxPooling1D(2)(x2); x2 = tf.keras.layers.Dropout(0.2)(x2)\n",
    "\n",
    "x2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33df75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(dataset, imu_dim, wd=1e-4):\n",
    "    sample_batch = next(iter(dataset))\n",
    "    input_shape = sample_batch[0].shape[1:]\n",
    "    inp = tf.keras.layers.Input(shape=input_shape)\n",
    "    imu = tf.keras.layers.Lambda(lambda t: t[:, :, :imu_dim])(inp)\n",
    "    tof = tf.keras.layers.Lambda(lambda t: t[:, :, imu_dim:])(inp)\n",
    "\n",
    "    # IMU deep branch\n",
    "    x1 = residual_se_cnn_block(imu, 64, 3, drop=0.1, wd=wd)\n",
    "    x1 = residual_se_cnn_block(x1, 128, 5, drop=0.1, wd=wd)\n",
    "\n",
    "    # TOF/Thermal lighter branch\n",
    "    x2 = tf.keras.layers.Conv1D(64, 3, padding='same', use_bias=False, kernel_regularizer=tf.keras.layers.l2(wd))(tof)\n",
    "    x2 = tf.keras.layers.BatchNormalization()(x2); x2 = tf.keras.layers.Activation('relu')(x2)\n",
    "    x2 = tf.keras.layers.MaxPooling1D(2)(x2); x2 = tf.keras.layers.Dropout(0.2)(x2)\n",
    "    x2 = tf.keras.layers.Conv1D(128, 3, padding='same', use_bias=False, kernel_regularizer=tf.keras.layers.l2(wd))(x2)\n",
    "    x2 = tf.keras.layers.BatchNormalization()(x2); x2 = tf.keras.layers.Activation('relu')(x2)\n",
    "    x2 = tf.keras.layers.MaxPooling1D(2)(x2); x2 = tf.keras.layers.Dropout(0.2)(x2)\n",
    "\n",
    "    merged = tf.keras.layers.Concatenate()([x1, x2])\n",
    "\n",
    "    xa = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True, kernel_regularizer=tf.keras.layers.l2(wd)))(merged)\n",
    "    xb = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(128, return_sequences=True, kernel_regularizer=tf.keras.layers.l2(wd)))(merged)\n",
    "    xc = tf.keras.layers.GaussianNoise(0.09)(merged)\n",
    "    xc = tf.keras.layers.Dense(16, activation='elu')(xc)\n",
    "    \n",
    "    x = tf.keras.layers.Concatenate()([xa, xb, xc])\n",
    "    x = tf.keras.layers.Dropout(0.4)(x)\n",
    "    x = attention_layer(x)\n",
    "\n",
    "    for units, drop in [(256, 0.5), (128, 0.3)]:\n",
    "        x = tf.keras.layers.Dense(units, use_bias=False, kernel_regularizer=tf.keras.layers.l2(wd))(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x); x = tf.keras.layers.Activation('relu')(x)\n",
    "        x = tf.keras.layers.Dropout(drop)(x)\n",
    "\n",
    "    main_out = tf.keras.layers.tf.keras.layers.Dense(18, activation=\"softmax\", name=\"main_output\")(x)\n",
    "    gate_out = tf.keras.layers.tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"tof_gate\")(x) # Renamed layer\n",
    "    \n",
    "    return tf.keras.models.Model(inputs=inp, outputs={\"main_output\": main_out, \"tof_gate\": gate_out})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e24c3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # =====================================================================================\n",
    "# # --- INFERENCE & LOCAL DEBUGGING SCRIPT ---\n",
    "# # =====================================================================================\n",
    "# import pandas as pd\n",
    "# import polars as pl\n",
    "# import numpy as np\n",
    "# import joblib\n",
    "# import traceback\n",
    "# from tensorflow.keras.models import load_model\n",
    "# from tensorflow.keras.utils import pad_sequences\n",
    "\n",
    "# import os\n",
    "# import gc\n",
    "# import joblib\n",
    "# import numpy as np\n",
    "# import polars as pl\n",
    "# import tensorflow as tf\n",
    "# from pathlib import Path\n",
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "# from sklearn.metrics import classification_report, accuracy_score\n",
    "# from tensorflow.keras.models import load_model\n",
    "# from tensorflow.keras.utils import pad_sequences, to_categorical\n",
    "# from tensorflow import argmax, minimum, shape\n",
    "\n",
    "# # --- Your existing function imports ---\n",
    "# from src.nn_blocks import (\n",
    "#     unet_se_cnn,\n",
    "#     features_processing, \n",
    "#     GatedMixupGenerator, \n",
    "#     tof_block, \n",
    "#     match_time_steps, \n",
    "#     time_sum, \n",
    "#     squeeze_last_axis,\n",
    "#     expand_last_axis,\n",
    "#     crop_or_pad_output_shape\n",
    "# )\n",
    "\n",
    "# from src.functions import (\n",
    "#     train_model, \n",
    "#     create_sequence_dataset,\n",
    "#     perform_padding,\n",
    "#     generate_gate_targets\n",
    "# )\n",
    "# from src.constants import DATA_PATH\n",
    "# from src.tof_feats import remove_gravity_from_acc, calculate_angular_velocity_from_quat, calculate_angular_distance\n",
    "\n",
    "# def crop_or_pad(inputs):\n",
    "#     x, skip = inputs\n",
    "#     x_len = shape(x)[1]\n",
    "#     skip_len = shape(skip)[1]\n",
    "#     min_len = minimum(x_len, skip_len)\n",
    "#     return x[:, :min_len, :], skip[:, :min_len, :]\n",
    "\n",
    "# # =====================================================================================\n",
    "# # MASTER CONTROL FLAG\n",
    "# # =====================================================================================\n",
    "# TRAIN = True \n",
    "# TRAIN = False\n",
    "\n",
    "# # =====================================================================================\n",
    "# # CONFIGURATION\n",
    "# # =====================================================================================\n",
    "# PARQUET_FILE = 'output/final_processed_train_data.parquet'\n",
    "# PRETRAINED_DIR = Path(\"output/artifacts\")\n",
    "# PRETRAINED_DIR.mkdir(parents=True, exist_ok=True) # Ensure directory exists\n",
    "\n",
    "# LR_INIT = 5e-4\n",
    "# WD = 3e-3\n",
    "# NUM_CLASSES = 18\n",
    "# BATCH_SIZE = 64\n",
    "# N_SPLITS = 4 \n",
    "# MAX_PAD_LEN = 128\n",
    "\n",
    "# # --- 2. Define TTA Parameters and Predict Function ---\n",
    "# TTA_STEPS = 10\n",
    "# TTA_NOISE_STDDEV = 0.01\n",
    "\n",
    "# # =====================================================================================\n",
    "# # MODEL DEFINITION (Your existing function)\n",
    "# # =====================================================================================\n",
    "# def create_model(dataset, imu_dim, wd=1e-4):\n",
    "#     sample_batch = next(iter(dataset))\n",
    "#     input_shape = sample_batch[0].shape[1:]\n",
    "#     inp = tf.keras.layers.Input(shape=input_shape)\n",
    "#     imu = tf.keras.layers.Lambda(lambda t: t[:, :, :imu_dim])(inp)\n",
    "#     tof = tf.keras.layers.Lambda(lambda t: t[:, :, imu_dim:])(inp)\n",
    "\n",
    "#     x1 = unet_se_cnn(imu, 3, base_filters=64, kernel_size=3)\n",
    "#     x2 = tof_block(tof, wd)\n",
    "\n",
    "#     x = features_processing(x1, x2)\n",
    "#     x = tf.keras.layers.Dropout(0.3)(x) \n",
    "#     main_out = tf.keras.layers.Dense(18, activation=\"softmax\", name=\"main_output\")(x)\n",
    "#     gate_out = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"tof_gate\")(x) # Renamed layer\n",
    "    \n",
    "#     return tf.keras.models.Model(inputs=inp, outputs={\"main_output\": main_out, \"tof_gate\": gate_out})\n",
    "\n",
    "# # --- 1. Load All Inference Artifacts ---\n",
    "# print(\"▶ LOCAL DEBUG MODE – loading artefacts from\", PRETRAINED_DIR)\n",
    "# try:\n",
    "#     final_feature_cols = np.load(PRETRAINED_DIR / \"feature_cols.npy\", allow_pickle=True).tolist()\n",
    "#     pad_len = int(np.load(PRETRAINED_DIR / \"sequence_maxlen.npy\"))\n",
    "#     scaler = joblib.load(PRETRAINED_DIR / \"scaler.pkl\")\n",
    "#     gesture_classes = np.load(PRETRAINED_DIR / \"gesture_classes.npy\", allow_pickle=True)\n",
    "\n",
    "#     models = []\n",
    "#     print(f\"  Loading {N_SPLITS} models for ensemble inference...\")\n",
    "#     for fold in range(N_SPLITS):\n",
    "#         model_path = PRETRAINED_DIR / f\"gesture_model_fold_{fold}.h5\"\n",
    "#         model = load_model(model_path, compile=False, custom_objects={\n",
    "#             'unet_se_cnn': unet_se_cnn,\n",
    "#             'tof_block': tof_block,\n",
    "#             'features_processing': features_processing,\n",
    "#             'match_time_steps': match_time_steps,\n",
    "#             'crop_or_pad': crop_or_pad,\n",
    "#             'squeeze_last_axis': squeeze_last_axis,\n",
    "#             'expand_last_axis': expand_last_axis,\n",
    "#             'time_sum': time_sum,\n",
    "#             'crop_or_pad_output_shape': crop_or_pad_output_shape\n",
    "#         })\n",
    "#         models.append(model)\n",
    "#     print(\"  Models, scaler, and metadata loaded.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"ERROR loading artifacts: {e}\")\n",
    "#     # Stop execution if artifacts can't be loaded\n",
    "#     exit()\n",
    "\n",
    "# # --- 2. Define the Predict Function (Using the most robust version) ---\n",
    "# def predict(sequence: pl.DataFrame, demographics: pl.DataFrame) -> str:\n",
    "#     # ... (All your feature engineering code is correct and can remain the same) ...\n",
    "#     df_seq = sequence.to_pandas()\n",
    "#     # ... (Sanitization, feature creation, scaling, padding) ...\n",
    "#     sensor_cols = [c for c in df_seq.columns if c.startswith(('acc_', 'rot_', 'thm_', 'tof_'))]\n",
    "#     for col in sensor_cols:\n",
    "#         if df_seq[col].dtype == 'object':\n",
    "#             df_seq[col] = pd.to_numeric(df_seq[col], errors='coerce')\n",
    "#     new_features = {}\n",
    "#     linear_accel = remove_gravity_from_acc(df_seq, df_seq)\n",
    "#     new_features['linear_acc_x'] = linear_accel[:, 0]\n",
    "#     new_features['linear_acc_y'] = linear_accel[:, 1]\n",
    "#     new_features['linear_acc_z'] = linear_accel[:, 2]\n",
    "#     linear_acc_mag = np.sqrt(np.square(linear_accel).sum(axis=1))\n",
    "#     new_features['linear_acc_mag'] = linear_acc_mag\n",
    "#     new_features['linear_acc_mag_jerk'] = pd.Series(linear_acc_mag).diff().fillna(0).values\n",
    "#     angular_vel = calculate_angular_velocity_from_quat(df_seq)\n",
    "#     new_features['angular_vel_x'] = angular_vel[:, 0]\n",
    "#     new_features['angular_vel_y'] = angular_vel[:, 1]\n",
    "#     new_features['angular_vel_z'] = angular_vel[:, 2]\n",
    "#     new_features['angular_distance'] = calculate_angular_distance(df_seq)\n",
    "#     for i in range(1, 6):\n",
    "#         pixel_cols = [f\"tof_{i}_v{p}\" for p in range(64)]\n",
    "#         tof_data = df_seq[pixel_cols].replace(-1, np.nan)\n",
    "#         new_features[f'tof_{i}_mean'] = tof_data.mean(axis=1)\n",
    "#         new_features[f'tof_{i}_std'] = tof_data.std(axis=1)\n",
    "#         new_features[f'tof_{i}_min'] = tof_data.min(axis=1)\n",
    "#         new_features[f'tof_{i}_max'] = tof_data.max(axis=1)\n",
    "#     df_seq = df_seq.assign(**new_features)\n",
    "#     mat_unscaled_df = df_seq[final_feature_cols].ffill().bfill().fillna(0)\n",
    "#     mat_scaled = scaler.transform(mat_unscaled_df)\n",
    "#     pad_input = pad_sequences([mat_scaled], maxlen=pad_len, padding='post', truncating='post', dtype='float32')\n",
    "\n",
    "#     # --- TTA Loop ---\n",
    "#     all_tta_predictions = []\n",
    "#     for i in range(TTA_STEPS):\n",
    "#         noisy_input = pad_input\n",
    "#         if i > 0:\n",
    "#             noise = tf.random.normal(shape=tf.shape(pad_input), mean=0.0, stddev=TTA_NOISE_STDDEV)\n",
    "#             noisy_input = pad_input + noise\n",
    "\n",
    "#         # Ensemble predictions from all fold models\n",
    "#         all_fold_predictions = []\n",
    "#         for model in models:\n",
    "            \n",
    "#             # =========================================================================\n",
    "#             # --- THE FINAL FIX IS HERE ---\n",
    "#             # =========================================================================\n",
    "#             # model.predict returns a dictionary, access the 'main_output' key\n",
    "#             predictions_dict = model.predict(noisy_input, verbose=0)\n",
    "#             main_preds = predictions_dict['main_output']\n",
    "            \n",
    "#             all_fold_predictions.append(main_preds)\n",
    "        \n",
    "#         avg_fold_prediction = np.mean(all_fold_predictions, axis=0)\n",
    "#         all_tta_predictions.append(avg_fold_prediction)\n",
    "\n",
    "#     # --- Final Averaging and Prediction (Unchanged) ---\n",
    "#     final_avg_prediction = np.mean(all_tta_predictions, axis=0)\n",
    "#     idx = int(final_avg_prediction.argmax())\n",
    "    \n",
    "#     return str(gesture_classes[idx])\n",
    "\n",
    "# # =====================================================================================\n",
    "# # --- LOCAL TEST HARNESS ---\n",
    "# # =====================================================================================\n",
    "# print(\"\\n--- Starting Local Test ---\")\n",
    "\n",
    "# # Load the actual test data\n",
    "# TEST_CSV_PATH = 'input/cmi-detect-behavior-with-sensor-data/test.csv'\n",
    "# TEST_DEM_PATH = 'input/cmi-detect-behavior-with-sensor-data/test_demographics.csv'\n",
    "\n",
    "# try:\n",
    "#     test_df = pl.read_csv(TEST_CSV_PATH)\n",
    "#     test_dem_df = pl.read_csv(TEST_DEM_PATH)\n",
    "    \n",
    "#     # Pick the first sequence from the test set\n",
    "#     target_sequence_id = test_df.get_column(\"sequence_id\").unique()[0]\n",
    "#     print(f\"Testing with sequence_id: {target_sequence_id}\")\n",
    "    \n",
    "#     # Isolate the data for that single sequence\n",
    "#     sample_sequence_pl = test_df.filter(pl.col(\"sequence_id\") == target_sequence_id)\n",
    "    \n",
    "#     # Find the corresponding subject and their demographics\n",
    "#     subject_id = sample_sequence_pl.get_column(\"subject\")[0]\n",
    "#     sample_demographics_pl = test_dem_df.filter(pl.col(\"subject\") == subject_id)\n",
    "    \n",
    "#     # --- Call the predict function directly and catch the REAL error ---\n",
    "#     print(\"\\nCalling predict function directly...\")\n",
    "#     predicted_gesture = predict(sample_sequence_pl, sample_demographics_pl)\n",
    "    \n",
    "#     print(\"\\n✅ SUCCESS! The function ran without errors on a sample.\")\n",
    "#     print(f\"Predicted Gesture: {predicted_gesture}\")\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(\"\\n❌ ERROR! The function failed. Here is the full Python traceback:\")\n",
    "#     traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
