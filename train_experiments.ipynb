{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea175bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting merge process...\n",
      "  Loading and joining features from: imu_basic_physics_feats.parquet\n",
      "  Loading and joining features from: tof_basic_kaggle_feats.parquet\n",
      "  Merge complete.\n",
      "  Final merged DataFrame created with shape: (574945, 41)\n",
      "\n",
      "=== Fold 1/4 ===\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from tensorflow.keras.utils import pad_sequences, to_categorical\n",
    "\n",
    "from src.nn_blocks import (\n",
    "    unet_se_cnn,\n",
    "    features_processing, \n",
    "    GatedMixupGenerator,\n",
    ")\n",
    "\n",
    "from src.merge_feats_dynamic import merge_feature_sets\n",
    "\n",
    "from src.functions import (\n",
    "    train_model, \n",
    "    create_sequence_dataset,\n",
    "    perform_padding,\n",
    "    generate_gate_targets\n",
    ")\n",
    "\n",
    "# =====================================================================================\n",
    "# CONFIGURATION\n",
    "# =====================================================================================\n",
    "\n",
    "LR_INIT = 5e-4\n",
    "WD = 3e-3\n",
    "NUM_CLASSES = 18\n",
    "BATCH_SIZE = 64\n",
    "N_SPLITS = 4 \n",
    "MAX_PAD_LEN = 128\n",
    "\n",
    "# =====================================================================================\n",
    "# MODEL DEFINITION (Your existing function)\n",
    "# =====================================================================================\n",
    "\n",
    "from src.nn_blocks import (\n",
    "    wave_block, residual_se_cnn_block, tof_block_2, attention_layer\n",
    ")\n",
    "\n",
    "def create_model(dataset, imu_dim, wd=1e-4):\n",
    "    sample_batch = next(iter(dataset))\n",
    "    input_shape = sample_batch[0].shape[1:]\n",
    "    inp = tf.keras.layers.Input(shape=input_shape)\n",
    "    imu = tf.keras.layers.Lambda(lambda t: t[:, :, :imu_dim])(inp)\n",
    "    tof = tf.keras.layers.Lambda(lambda t: t[:, :, imu_dim:])(inp)\n",
    "\n",
    "    xa = unet_se_cnn(imu, 3, base_filters=128, kernel_size=3) # 64,128\n",
    "    xa = unet_se_cnn(xa, 3, base_filters=128, kernel_size=5)\n",
    "    # xb = tf.keras.layers.MaxPool1D(2)(xb) # 64,128\n",
    "    # x1 = tf.keras.layers.Concatenate()([xa, xb])\n",
    "    # x1 = tf.keras.layers.Conv1D(filters=128, kernel_size=3, strides=2, padding='same', activation='relu')(x1)\n",
    "\n",
    "    # input_shape=[(None, 64, 256), (None, 32, 128)\n",
    "    x2 = tof_block_2(tof, wd) \n",
    "\n",
    "    x = features_processing(xa, x2)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x) \n",
    "    main_out = tf.keras.layers.Dense(18, activation=\"softmax\", name=\"main_output\")(x)\n",
    "    gate_out = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"tof_gate\")(x) # Renamed layer\n",
    "    \n",
    "    return tf.keras.models.Model(inputs=inp, outputs={\"main_output\": main_out, \"tof_gate\": gate_out})\n",
    "\n",
    "# =====================================================================================\n",
    "# TRAINING LOGIC\n",
    "# =====================================================================================\n",
    "\n",
    "FEATURE_DIR = Path('output')\n",
    "RAW_DIR = Path('input/cmi-detect-behavior-with-sensor-data')\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "files_to_merge = [\n",
    "    # \"imu_physics_feats.parquet\",\n",
    "    # \"imu_rolling_stats_features.parquet\",\n",
    "    # \"imu_cross_modal_features.parquet\",\n",
    "    # 'output/kaggle_0.8_feats.parquet',\n",
    "    'imu_basic_physics_feats.parquet',\n",
    "    'tof_basic_kaggle_feats.parquet'\n",
    "    ]\n",
    "\n",
    "feature_paths = [FEATURE_DIR / f for f in files_to_merge]\n",
    "\n",
    "base_df = pl.read_parquet(FEATURE_DIR / \"cleaned_base_train_data.parquet\")\n",
    "demographics_df = pl.read_csv(RAW_DIR / \"train_demographics.csv\")\n",
    "base_df = base_df.join(demographics_df, on='subject', how='left')\n",
    "\n",
    "all_raw_columns = base_df.columns\n",
    "meta_cols = ['sequence_id', 'sequence_counter', 'subject', 'gesture']\n",
    "raw_imu_cols = [c for c in all_raw_columns if c.startswith(('acc_', 'rot_'))]\n",
    "base_df = base_df.select(meta_cols)\n",
    "\n",
    "le = LabelEncoder()\n",
    "gesture_encoded = le.fit_transform(base_df.get_column('gesture'))\n",
    "base_df = base_df.with_columns(pl.Series(\"gesture_int\", gesture_encoded))  \n",
    "\n",
    "final_df = merge_feature_sets(base_df, feature_paths)\n",
    "print(f\"  Final merged DataFrame created with shape: {final_df.shape}\")\n",
    "\n",
    "all_final_columns = final_df.columns\n",
    "# Define all columns that are NOT features for the model\n",
    "final_meta_cols = {'gesture', 'gesture_int', 'sequence_type', 'behavior', 'orientation',\n",
    "                    'row_id', 'subject', 'phase', 'sequence_id', 'sequence_counter'}\n",
    "demographic_cols = {'adult_child', 'age', 'sex', 'handedness', 'height_cm', 'shoulder_to_wrist_cm', 'elbow_to_wrist_cm'}\n",
    "\n",
    "# This is the final list of columns to be scaled and fed to the model\n",
    "all_feats = [c for c in all_final_columns if c not in final_meta_cols and c not in demographic_cols]\n",
    "\n",
    "imu_cols = ['acc_x', 'acc_y', 'acc_z', 'rot_w',\n",
    "       'rot_x', 'rot_y', 'rot_z', 'linear_acc_x', 'linear_acc_y',\n",
    "       'linear_acc_z', 'linear_acc_mag', 'linear_acc_mag_jerk',\n",
    "       'angular_vel_x', 'angular_vel_y', 'angular_vel_z', 'angular_distance']\n",
    "\n",
    "tof_cols = ['tof_1_mean', 'tof_1_std',\n",
    "       'tof_1_min', 'tof_1_max', 'tof_2_mean', 'tof_2_std', 'tof_2_min',\n",
    "       'tof_2_max', 'tof_3_mean', 'tof_3_std', 'tof_3_min', 'tof_3_max',\n",
    "       'tof_4_mean', 'tof_4_std', 'tof_4_min', 'tof_4_max', 'tof_5_mean',\n",
    "       'tof_5_std', 'tof_5_min', 'tof_5_max']\n",
    "\n",
    "imu_dim = len(imu_cols)\n",
    "\n",
    "cv_info = final_df.group_by(\"sequence_id\").agg(pl.first(\"gesture_int\")).sort(\"sequence_id\")\n",
    "all_sequence_ids = cv_info.get_column(\"sequence_id\").to_numpy()\n",
    "y_for_split = cv_info.get_column(\"gesture_int\").to_numpy()\n",
    "\n",
    "kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "fold_accuracies = []\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "for fold_idx, (train_indices, val_indices) in enumerate(kf.split(all_sequence_ids, y_for_split)):\n",
    "    print(f\"\\n=== Fold {fold_idx + 1}/{N_SPLITS} ===\")\n",
    "    train_ids = all_sequence_ids[train_indices]\n",
    "    val_ids = all_sequence_ids[val_indices]\n",
    "\n",
    "    # Filter the merged DataFrame for the current fold\n",
    "    train_df = final_df.filter(pl.col('sequence_id').is_in(train_ids))\n",
    "    val_df = final_df.filter(pl.col('sequence_id').is_in(val_ids))\n",
    "    \n",
    "    train_gate_df = generate_gate_targets(train_df, tof_cols)\n",
    "    val_gate_df = generate_gate_targets(val_df, tof_cols)    \n",
    "\n",
    "    le = LabelEncoder().fit(train_df['gesture'])\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    train_features_scaled = scaler.fit_transform(train_df[all_feats])\n",
    "    val_features_scaled = scaler.transform(val_df[all_feats])\n",
    "    X_train_scaled_features = pl.DataFrame(train_features_scaled, schema=all_feats)\n",
    "    X_val_scaled_features = pl.DataFrame(val_features_scaled, schema=all_feats)\n",
    "\n",
    "    meta_cols_to_keep = ['sequence_id', 'sequence_counter', 'gesture_int']\n",
    "    train_df_final = train_df.select(meta_cols_to_keep).with_columns(X_train_scaled_features)\n",
    "    val_df_final = val_df.select(meta_cols_to_keep).with_columns(X_val_scaled_features)\n",
    "\n",
    "    del train_df, val_df, X_train_scaled_features, X_val_scaled_features\n",
    "    gc.collect()\n",
    "\n",
    "    X_train, y_train, train_gate_target = create_sequence_dataset(train_df_final, imu_cols + tof_cols, train_gate_df)\n",
    "    X_val, y_val, val_gate_target = create_sequence_dataset(val_df_final, imu_cols + tof_cols, val_gate_df)\n",
    "\n",
    "    del train_df_final, val_df_final\n",
    "    gc.collect()\n",
    "\n",
    "    X_train_padded = perform_padding(X_train, MAX_PAD_LEN)\n",
    "    X_val_padded = perform_padding(X_val, MAX_PAD_LEN)\n",
    "    \n",
    "    y_train_cat = to_categorical(y_train, num_classes=NUM_CLASSES)\n",
    "    y_val_cat = to_categorical(y_val, num_classes=NUM_CLASSES)\n",
    "\n",
    "    train_dataset = GatedMixupGenerator(\n",
    "        X=X_train_padded, y=y_train_cat, gate_targets=train_gate_target,\n",
    "        batch_size=BATCH_SIZE, imu_dim=imu_dim, alpha=0.2, masking_prob=0.25\n",
    "    )\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        X_val_padded, {'main_output': y_val_cat, 'tof_gate': val_gate_target[:, np.newaxis]}\n",
    "    )).batch(BATCH_SIZE).cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    del X_val, y_val, X_train, y_train, X_train_padded, X_val_padded\n",
    "    gc.collect()\n",
    "    \n",
    "    model = create_model(train_dataset, len(imu_cols))\n",
    "    train_model(model, train_dataset, val_dataset, 150, LR_INIT, WD)\n",
    "\n",
    "    # --- EVALUATION ---\n",
    "    val_preds = model.predict(val_dataset)\n",
    "    main_output_preds = val_preds['main_output']\n",
    "    y_pred_fold = np.argmax(main_output_preds, axis=1)\n",
    "    y_true_fold = np.argmax(y_val_cat, axis=1)\n",
    "    fold_acc = accuracy_score(y_true_fold, y_pred_fold)\n",
    "    fold_accuracies.append(fold_acc)\n",
    "    print(f\"Fold {fold_idx + 1} Accuracy: {fold_acc:.4f}\")\n",
    "    all_preds.append(y_pred_fold)\n",
    "    all_labels.append(y_true_fold)\n",
    "\n",
    "    del train_dataset, model, val_dataset\n",
    "    gc.collect()\n",
    "\n",
    "    # --- FINAL OOF REPORT ---\n",
    "    print(\"\\n=== Cross-validation Summary ===\")\n",
    "    print(f\"Per-fold Accuracies: {fold_accuracies}\")\n",
    "    print(f\"Mean Accuracy: {np.mean(fold_accuracies):.4f} ± {np.std(fold_accuracies):.4f}\")\n",
    "    y_all_pred = np.concatenate(all_preds)\n",
    "    y_all_true = np.concatenate(all_labels)\n",
    "    print(\"\\n=== Overall Classification Report ===\")\n",
    "    print(classification_report(y_all_true, y_all_pred, target_names=le.classes_, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7c2f4d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sequence_id',\n",
       " 'sequence_counter',\n",
       " 'acc_x',\n",
       " 'acc_y',\n",
       " 'acc_z',\n",
       " 'rot_w',\n",
       " 'rot_x',\n",
       " 'rot_y',\n",
       " 'rot_z',\n",
       " 'linear_acc_x',\n",
       " 'linear_acc_y',\n",
       " 'linear_acc_z',\n",
       " 'linear_acc_mag',\n",
       " 'linear_acc_mag_jerk',\n",
       " 'angular_vel_x',\n",
       " 'angular_vel_y',\n",
       " 'angular_vel_z',\n",
       " 'angular_distance']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imu_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e5febb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sequence_id',\n",
       " 'sequence_counter',\n",
       " 'acc_x',\n",
       " 'acc_y',\n",
       " 'acc_z',\n",
       " 'rot_w',\n",
       " 'rot_x',\n",
       " 'rot_y',\n",
       " 'rot_z',\n",
       " 'linear_acc_x',\n",
       " 'linear_acc_y',\n",
       " 'linear_acc_z',\n",
       " 'linear_acc_mag',\n",
       " 'linear_acc_mag_jerk',\n",
       " 'angular_vel_x',\n",
       " 'angular_vel_y',\n",
       " 'angular_vel_z',\n",
       " 'angular_distance',\n",
       " 'tof_1_mean',\n",
       " 'tof_1_std',\n",
       " 'tof_1_min',\n",
       " 'tof_1_max',\n",
       " 'tof_2_mean',\n",
       " 'tof_2_std',\n",
       " 'tof_2_min',\n",
       " 'tof_2_max',\n",
       " 'tof_3_mean',\n",
       " 'tof_3_std',\n",
       " 'tof_3_min',\n",
       " 'tof_3_max',\n",
       " 'tof_4_mean',\n",
       " 'tof_4_std',\n",
       " 'tof_4_min',\n",
       " 'tof_4_max',\n",
       " 'tof_5_mean',\n",
       " 'tof_5_std',\n",
       " 'tof_5_min',\n",
       " 'tof_5_max']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imu_cols + tof_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73f8589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "PARQUET_FILE = 'output/kaggle_0.8_feats.parquet'\n",
    "df = pl.read_parquet(PARQUET_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30587df",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrain_dataset\u001b[49m:\n\u001b[32m      2\u001b[39m     x = e[\u001b[32m0\u001b[39m]\n\u001b[32m      3\u001b[39m     y = e[\u001b[32m1\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "for e in train_dataset:\n",
    "    x = e[0]\n",
    "    y = e[1]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d3ad30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 128, 38)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f2ecb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 38)\n"
     ]
    }
   ],
   "source": [
    "input_shape = x[0].shape\n",
    "inp = tf.keras.layers.Input(shape=input_shape)\n",
    "imu = tf.keras.layers.Lambda(lambda t: t[:, :, :imu_dim])(inp)\n",
    "tof = tf.keras.layers.Lambda(lambda t: t[:, :, imu_dim:])(inp)\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7a87a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 32, 128)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wd = 0\n",
    "# TOF/Thermal lighter branch\n",
    "x2 = tf.keras.layers.Conv1D(64, 3, padding='same', use_bias=False, kernel_regularizer=tf.keras.regularizers.l2(wd))(tof)\n",
    "x2 = tf.keras.layers.BatchNormalization()(x2); x2 = tf.keras.layers.Activation('relu')(x2)\n",
    "x2 = tf.keras.layers.MaxPooling1D(2)(x2); x2 = tf.keras.layers.Dropout(0.2)(x2)\n",
    "x2 = tf.keras.layers.Conv1D(128, 3, padding='same', use_bias=False, kernel_regularizer=tf.keras.regularizers.l2(wd))(x2)\n",
    "x2 = tf.keras.layers.BatchNormalization()(x2); x2 = tf.keras.layers.Activation('relu')(x2)\n",
    "x2 = tf.keras.layers.MaxPooling1D(2)(x2); x2 = tf.keras.layers.Dropout(0.2)(x2)\n",
    "\n",
    "x2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33df75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(dataset, imu_dim, wd=1e-4):\n",
    "    sample_batch = next(iter(dataset))\n",
    "    input_shape = sample_batch[0].shape[1:]\n",
    "    inp = tf.keras.layers.Input(shape=input_shape)\n",
    "    imu = tf.keras.layers.Lambda(lambda t: t[:, :, :imu_dim])(inp)\n",
    "    tof = tf.keras.layers.Lambda(lambda t: t[:, :, imu_dim:])(inp)\n",
    "\n",
    "    # IMU deep branch\n",
    "    x1 = residual_se_cnn_block(imu, 64, 3, drop=0.1, wd=wd)\n",
    "    x1 = residual_se_cnn_block(x1, 128, 5, drop=0.1, wd=wd)\n",
    "\n",
    "    # TOF/Thermal lighter branch\n",
    "    x2 = tf.keras.layers.Conv1D(64, 3, padding='same', use_bias=False, kernel_regularizer=tf.keras.layers.l2(wd))(tof)\n",
    "    x2 = tf.keras.layers.BatchNormalization()(x2); x2 = tf.keras.layers.Activation('relu')(x2)\n",
    "    x2 = tf.keras.layers.MaxPooling1D(2)(x2); x2 = tf.keras.layers.Dropout(0.2)(x2)\n",
    "    x2 = tf.keras.layers.Conv1D(128, 3, padding='same', use_bias=False, kernel_regularizer=tf.keras.layers.l2(wd))(x2)\n",
    "    x2 = tf.keras.layers.BatchNormalization()(x2); x2 = tf.keras.layers.Activation('relu')(x2)\n",
    "    x2 = tf.keras.layers.MaxPooling1D(2)(x2); x2 = tf.keras.layers.Dropout(0.2)(x2)\n",
    "\n",
    "    merged = tf.keras.layers.Concatenate()([x1, x2])\n",
    "\n",
    "    xa = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True, kernel_regularizer=tf.keras.layers.l2(wd)))(merged)\n",
    "    xb = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(128, return_sequences=True, kernel_regularizer=tf.keras.layers.l2(wd)))(merged)\n",
    "    xc = tf.keras.layers.GaussianNoise(0.09)(merged)\n",
    "    xc = tf.keras.layers.Dense(16, activation='elu')(xc)\n",
    "    \n",
    "    x = tf.keras.layers.Concatenate()([xa, xb, xc])\n",
    "    x = tf.keras.layers.Dropout(0.4)(x)\n",
    "    x = attention_layer(x)\n",
    "\n",
    "    for units, drop in [(256, 0.5), (128, 0.3)]:\n",
    "        x = tf.keras.layers.Dense(units, use_bias=False, kernel_regularizer=tf.keras.layers.l2(wd))(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x); x = tf.keras.layers.Activation('relu')(x)\n",
    "        x = tf.keras.layers.Dropout(drop)(x)\n",
    "\n",
    "    main_out = tf.keras.layers.tf.keras.layers.Dense(18, activation=\"softmax\", name=\"main_output\")(x)\n",
    "    gate_out = tf.keras.layers.tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"tof_gate\")(x) # Renamed layer\n",
    "    \n",
    "    return tf.keras.models.Model(inputs=inp, outputs={\"main_output\": main_out, \"tof_gate\": gate_out})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e24c3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # =====================================================================================\n",
    "# # --- INFERENCE & LOCAL DEBUGGING SCRIPT ---\n",
    "# # =====================================================================================\n",
    "# import pandas as pd\n",
    "# import polars as pl\n",
    "# import numpy as np\n",
    "# import joblib\n",
    "# import traceback\n",
    "# from tensorflow.keras.models import load_model\n",
    "# from tensorflow.keras.utils import pad_sequences\n",
    "\n",
    "# import os\n",
    "# import gc\n",
    "# import joblib\n",
    "# import numpy as np\n",
    "# import polars as pl\n",
    "# import tensorflow as tf\n",
    "# from pathlib import Path\n",
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "# from sklearn.metrics import classification_report, accuracy_score\n",
    "# from tensorflow.keras.models import load_model\n",
    "# from tensorflow.keras.utils import pad_sequences, to_categorical\n",
    "# from tensorflow import argmax, minimum, shape\n",
    "\n",
    "# # --- Your existing function imports ---\n",
    "# from src.nn_blocks import (\n",
    "#     unet_se_cnn,\n",
    "#     features_processing, \n",
    "#     GatedMixupGenerator, \n",
    "#     tof_block, \n",
    "#     match_time_steps, \n",
    "#     time_sum, \n",
    "#     squeeze_last_axis,\n",
    "#     expand_last_axis,\n",
    "#     crop_or_pad_output_shape\n",
    "# )\n",
    "\n",
    "# from src.functions import (\n",
    "#     train_model, \n",
    "#     create_sequence_dataset,\n",
    "#     perform_padding,\n",
    "#     generate_gate_targets\n",
    "# )\n",
    "# from src.constants import DATA_PATH\n",
    "# from src.tof_feats import remove_gravity_from_acc, calculate_angular_velocity_from_quat, calculate_angular_distance\n",
    "\n",
    "# def crop_or_pad(inputs):\n",
    "#     x, skip = inputs\n",
    "#     x_len = shape(x)[1]\n",
    "#     skip_len = shape(skip)[1]\n",
    "#     min_len = minimum(x_len, skip_len)\n",
    "#     return x[:, :min_len, :], skip[:, :min_len, :]\n",
    "\n",
    "# # =====================================================================================\n",
    "# # MASTER CONTROL FLAG\n",
    "# # =====================================================================================\n",
    "# TRAIN = True \n",
    "# TRAIN = False\n",
    "\n",
    "# # =====================================================================================\n",
    "# # CONFIGURATION\n",
    "# # =====================================================================================\n",
    "# PARQUET_FILE = 'output/final_processed_train_data.parquet'\n",
    "# PRETRAINED_DIR = Path(\"output/artifacts\")\n",
    "# PRETRAINED_DIR.mkdir(parents=True, exist_ok=True) # Ensure directory exists\n",
    "\n",
    "# LR_INIT = 5e-4\n",
    "# WD = 3e-3\n",
    "# NUM_CLASSES = 18\n",
    "# BATCH_SIZE = 64\n",
    "# N_SPLITS = 4 \n",
    "# MAX_PAD_LEN = 128\n",
    "\n",
    "# # --- 2. Define TTA Parameters and Predict Function ---\n",
    "# TTA_STEPS = 10\n",
    "# TTA_NOISE_STDDEV = 0.01\n",
    "\n",
    "# # =====================================================================================\n",
    "# # MODEL DEFINITION (Your existing function)\n",
    "# # =====================================================================================\n",
    "# def create_model(dataset, imu_dim, wd=1e-4):\n",
    "#     sample_batch = next(iter(dataset))\n",
    "#     input_shape = sample_batch[0].shape[1:]\n",
    "#     inp = tf.keras.layers.Input(shape=input_shape)\n",
    "#     imu = tf.keras.layers.Lambda(lambda t: t[:, :, :imu_dim])(inp)\n",
    "#     tof = tf.keras.layers.Lambda(lambda t: t[:, :, imu_dim:])(inp)\n",
    "\n",
    "#     x1 = unet_se_cnn(imu, 3, base_filters=64, kernel_size=3)\n",
    "#     x2 = tof_block(tof, wd)\n",
    "\n",
    "#     x = features_processing(x1, x2)\n",
    "#     x = tf.keras.layers.Dropout(0.3)(x) \n",
    "#     main_out = tf.keras.layers.Dense(18, activation=\"softmax\", name=\"main_output\")(x)\n",
    "#     gate_out = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"tof_gate\")(x) # Renamed layer\n",
    "    \n",
    "#     return tf.keras.models.Model(inputs=inp, outputs={\"main_output\": main_out, \"tof_gate\": gate_out})\n",
    "\n",
    "# # --- 1. Load All Inference Artifacts ---\n",
    "# print(\"▶ LOCAL DEBUG MODE – loading artefacts from\", PRETRAINED_DIR)\n",
    "# try:\n",
    "#     final_feature_cols = np.load(PRETRAINED_DIR / \"feature_cols.npy\", allow_pickle=True).tolist()\n",
    "#     pad_len = int(np.load(PRETRAINED_DIR / \"sequence_maxlen.npy\"))\n",
    "#     scaler = joblib.load(PRETRAINED_DIR / \"scaler.pkl\")\n",
    "#     gesture_classes = np.load(PRETRAINED_DIR / \"gesture_classes.npy\", allow_pickle=True)\n",
    "\n",
    "#     models = []\n",
    "#     print(f\"  Loading {N_SPLITS} models for ensemble inference...\")\n",
    "#     for fold in range(N_SPLITS):\n",
    "#         model_path = PRETRAINED_DIR / f\"gesture_model_fold_{fold}.h5\"\n",
    "#         model = load_model(model_path, compile=False, custom_objects={\n",
    "#             'unet_se_cnn': unet_se_cnn,\n",
    "#             'tof_block': tof_block,\n",
    "#             'features_processing': features_processing,\n",
    "#             'match_time_steps': match_time_steps,\n",
    "#             'crop_or_pad': crop_or_pad,\n",
    "#             'squeeze_last_axis': squeeze_last_axis,\n",
    "#             'expand_last_axis': expand_last_axis,\n",
    "#             'time_sum': time_sum,\n",
    "#             'crop_or_pad_output_shape': crop_or_pad_output_shape\n",
    "#         })\n",
    "#         models.append(model)\n",
    "#     print(\"  Models, scaler, and metadata loaded.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"ERROR loading artifacts: {e}\")\n",
    "#     # Stop execution if artifacts can't be loaded\n",
    "#     exit()\n",
    "\n",
    "# # --- 2. Define the Predict Function (Using the most robust version) ---\n",
    "# def predict(sequence: pl.DataFrame, demographics: pl.DataFrame) -> str:\n",
    "#     # ... (All your feature engineering code is correct and can remain the same) ...\n",
    "#     df_seq = sequence.to_pandas()\n",
    "#     # ... (Sanitization, feature creation, scaling, padding) ...\n",
    "#     sensor_cols = [c for c in df_seq.columns if c.startswith(('acc_', 'rot_', 'thm_', 'tof_'))]\n",
    "#     for col in sensor_cols:\n",
    "#         if df_seq[col].dtype == 'object':\n",
    "#             df_seq[col] = pd.to_numeric(df_seq[col], errors='coerce')\n",
    "#     new_features = {}\n",
    "#     linear_accel = remove_gravity_from_acc(df_seq, df_seq)\n",
    "#     new_features['linear_acc_x'] = linear_accel[:, 0]\n",
    "#     new_features['linear_acc_y'] = linear_accel[:, 1]\n",
    "#     new_features['linear_acc_z'] = linear_accel[:, 2]\n",
    "#     linear_acc_mag = np.sqrt(np.square(linear_accel).sum(axis=1))\n",
    "#     new_features['linear_acc_mag'] = linear_acc_mag\n",
    "#     new_features['linear_acc_mag_jerk'] = pd.Series(linear_acc_mag).diff().fillna(0).values\n",
    "#     angular_vel = calculate_angular_velocity_from_quat(df_seq)\n",
    "#     new_features['angular_vel_x'] = angular_vel[:, 0]\n",
    "#     new_features['angular_vel_y'] = angular_vel[:, 1]\n",
    "#     new_features['angular_vel_z'] = angular_vel[:, 2]\n",
    "#     new_features['angular_distance'] = calculate_angular_distance(df_seq)\n",
    "#     for i in range(1, 6):\n",
    "#         pixel_cols = [f\"tof_{i}_v{p}\" for p in range(64)]\n",
    "#         tof_data = df_seq[pixel_cols].replace(-1, np.nan)\n",
    "#         new_features[f'tof_{i}_mean'] = tof_data.mean(axis=1)\n",
    "#         new_features[f'tof_{i}_std'] = tof_data.std(axis=1)\n",
    "#         new_features[f'tof_{i}_min'] = tof_data.min(axis=1)\n",
    "#         new_features[f'tof_{i}_max'] = tof_data.max(axis=1)\n",
    "#     df_seq = df_seq.assign(**new_features)\n",
    "#     mat_unscaled_df = df_seq[final_feature_cols].ffill().bfill().fillna(0)\n",
    "#     mat_scaled = scaler.transform(mat_unscaled_df)\n",
    "#     pad_input = pad_sequences([mat_scaled], maxlen=pad_len, padding='post', truncating='post', dtype='float32')\n",
    "\n",
    "#     # --- TTA Loop ---\n",
    "#     all_tta_predictions = []\n",
    "#     for i in range(TTA_STEPS):\n",
    "#         noisy_input = pad_input\n",
    "#         if i > 0:\n",
    "#             noise = tf.random.normal(shape=tf.shape(pad_input), mean=0.0, stddev=TTA_NOISE_STDDEV)\n",
    "#             noisy_input = pad_input + noise\n",
    "\n",
    "#         # Ensemble predictions from all fold models\n",
    "#         all_fold_predictions = []\n",
    "#         for model in models:\n",
    "            \n",
    "#             # =========================================================================\n",
    "#             # --- THE FINAL FIX IS HERE ---\n",
    "#             # =========================================================================\n",
    "#             # model.predict returns a dictionary, access the 'main_output' key\n",
    "#             predictions_dict = model.predict(noisy_input, verbose=0)\n",
    "#             main_preds = predictions_dict['main_output']\n",
    "            \n",
    "#             all_fold_predictions.append(main_preds)\n",
    "        \n",
    "#         avg_fold_prediction = np.mean(all_fold_predictions, axis=0)\n",
    "#         all_tta_predictions.append(avg_fold_prediction)\n",
    "\n",
    "#     # --- Final Averaging and Prediction (Unchanged) ---\n",
    "#     final_avg_prediction = np.mean(all_tta_predictions, axis=0)\n",
    "#     idx = int(final_avg_prediction.argmax())\n",
    "    \n",
    "#     return str(gesture_classes[idx])\n",
    "\n",
    "# # =====================================================================================\n",
    "# # --- LOCAL TEST HARNESS ---\n",
    "# # =====================================================================================\n",
    "# print(\"\\n--- Starting Local Test ---\")\n",
    "\n",
    "# # Load the actual test data\n",
    "# TEST_CSV_PATH = 'input/cmi-detect-behavior-with-sensor-data/test.csv'\n",
    "# TEST_DEM_PATH = 'input/cmi-detect-behavior-with-sensor-data/test_demographics.csv'\n",
    "\n",
    "# try:\n",
    "#     test_df = pl.read_csv(TEST_CSV_PATH)\n",
    "#     test_dem_df = pl.read_csv(TEST_DEM_PATH)\n",
    "    \n",
    "#     # Pick the first sequence from the test set\n",
    "#     target_sequence_id = test_df.get_column(\"sequence_id\").unique()[0]\n",
    "#     print(f\"Testing with sequence_id: {target_sequence_id}\")\n",
    "    \n",
    "#     # Isolate the data for that single sequence\n",
    "#     sample_sequence_pl = test_df.filter(pl.col(\"sequence_id\") == target_sequence_id)\n",
    "    \n",
    "#     # Find the corresponding subject and their demographics\n",
    "#     subject_id = sample_sequence_pl.get_column(\"subject\")[0]\n",
    "#     sample_demographics_pl = test_dem_df.filter(pl.col(\"subject\") == subject_id)\n",
    "    \n",
    "#     # --- Call the predict function directly and catch the REAL error ---\n",
    "#     print(\"\\nCalling predict function directly...\")\n",
    "#     predicted_gesture = predict(sample_sequence_pl, sample_demographics_pl)\n",
    "    \n",
    "#     print(\"\\n✅ SUCCESS! The function ran without errors on a sample.\")\n",
    "#     print(f\"Predicted Gesture: {predicted_gesture}\")\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(\"\\n❌ ERROR! The function failed. Here is the full Python traceback:\")\n",
    "#     traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
