{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b33dec33",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-26T00:46:56.279113Z",
     "iopub.status.busy": "2025-06-26T00:46:56.278570Z",
     "iopub.status.idle": "2025-06-26T00:47:14.084972Z",
     "shell.execute_reply": "2025-06-26T00:47:14.084122Z"
    },
    "papermill": {
     "duration": 17.812968,
     "end_time": "2025-06-26T00:47:14.086655",
     "exception": false,
     "start_time": "2025-06-26T00:46:56.273687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-08 21:42:36.496377: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754685756.515512   97980 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754685756.521000   97980 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1754685756.538160   97980 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754685756.538192   97980 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754685756.538194   97980 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754685756.538196   97980 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-08 21:42:36.545020: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os, json, joblib, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from tensorflow.keras.utils import Sequence, to_categorical, pad_sequences\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv1D, BatchNormalization, Activation, add, MaxPooling1D, Dropout,\n",
    "    Bidirectional, LSTM, GlobalAveragePooling1D, Dense, Multiply, Reshape,\n",
    "    Lambda, Concatenate, GRU, GaussianNoise\n",
    ")\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "import polars as pl\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf35fdb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T00:47:14.094183Z",
     "iopub.status.busy": "2025-06-26T00:47:14.093450Z",
     "iopub.status.idle": "2025-06-26T00:47:14.098792Z",
     "shell.execute_reply": "2025-06-26T00:47:14.097988Z"
    },
    "papermill": {
     "duration": 0.010115,
     "end_time": "2025-06-26T00:47:14.100154",
     "exception": false,
     "start_time": "2025-06-26T00:47:14.090039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def seed_everything(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    tf.experimental.numpy.random.seed(seed)\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa6c0bef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T00:47:14.106656Z",
     "iopub.status.busy": "2025-06-26T00:47:14.106414Z",
     "iopub.status.idle": "2025-06-26T00:47:14.111689Z",
     "shell.execute_reply": "2025-06-26T00:47:14.110995Z"
    },
    "papermill": {
     "duration": 0.009966,
     "end_time": "2025-06-26T00:47:14.112940",
     "exception": false,
     "start_time": "2025-06-26T00:47:14.102974",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ imports ready · tensorflow 2.19.0\n"
     ]
    }
   ],
   "source": [
    "# (Competition metric will only be imported when TRAINing)\n",
    "TRAIN = False                \n",
    "DEBUG_GATE = False  # <-- [新機能] デバッグモードを有効化\n",
    "                     \n",
    "# RAW_DIR = Path(\"/kaggle/input/cmi-detect-behavior-with-sensor-data\")\n",
    "# RAW_DIR = Path(\"/input/cmi-detect-behavior-with-sensor-data\")\n",
    "# PRETRAINED_DIR = Path(\"/kaggle/input/cmi-pretrained-model-v5\")\n",
    "# EXPORT_DIR = Path(\"./\")\n",
    "RAW_DIR = Path(\"input/cmi-detect-behavior-with-sensor-data\")\n",
    "# PRETRAINED_DIR = Path(\"/kaggle/input/cmi-pretrained-model-v5\")\n",
    "EXPORT_DIR = Path(\"./\")\n",
    "BATCH_SIZE = 64\n",
    "PAD_PERCENTILE = 95\n",
    "LR_INIT = 5e-4\n",
    "WD = 3e-3\n",
    "MIXUP_ALPHA = 0.4\n",
    "EPOCHS = 160\n",
    "PATIENCE = 40\n",
    "N_SPLITS = 5\n",
    "MASKING_PROB = 0.25 # <-- [新機能] 学習時にTOF/THMをマスクする確率\n",
    "GATE_LOSS_WEIGHT = 0.2\n",
    "\n",
    "print(\"▶ imports ready · tensorflow\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46469ab9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T00:47:14.119610Z",
     "iopub.status.busy": "2025-06-26T00:47:14.119382Z",
     "iopub.status.idle": "2025-06-26T00:47:14.128624Z",
     "shell.execute_reply": "2025-06-26T00:47:14.128026Z"
    },
    "papermill": {
     "duration": 0.01383,
     "end_time": "2025-06-26T00:47:14.129757",
     "exception": false,
     "start_time": "2025-06-26T00:47:14.115927",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_gravity_from_acc(acc_data, rot_data):\n",
    "    acc_values = acc_data[['acc_x', 'acc_y', 'acc_z']].values\n",
    "    quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    linear_accel = np.zeros_like(acc_values)\n",
    "    gravity_world = np.array([0, 0, 9.81])\n",
    "    for i in range(len(acc_values)):\n",
    "        if np.all(np.isnan(quat_values[i])) or np.all(np.isclose(quat_values[i], 0)):\n",
    "            linear_accel[i, :] = acc_values[i, :]\n",
    "            continue\n",
    "        try:\n",
    "            rotation = R.from_quat(quat_values[i])\n",
    "            gravity_sensor_frame = rotation.apply(gravity_world, inverse=True)\n",
    "            linear_accel[i, :] = acc_values[i, :] - gravity_sensor_frame\n",
    "        except ValueError:\n",
    "             linear_accel[i, :] = acc_values[i, :]\n",
    "    return linear_accel\n",
    "\n",
    "def calculate_angular_velocity_from_quat(rot_data, time_delta=1/200):\n",
    "    quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    angular_vel = np.zeros((len(quat_values), 3))\n",
    "    for i in range(len(quat_values) - 1):\n",
    "        q_t, q_t_plus_dt = quat_values[i], quat_values[i+1]\n",
    "        if np.all(np.isnan(q_t)) or np.all(np.isnan(q_t_plus_dt)): continue\n",
    "        try:\n",
    "            rot_t = R.from_quat(q_t)\n",
    "            rot_t_plus_dt = R.from_quat(q_t_plus_dt)\n",
    "            delta_rot = rot_t.inv() * rot_t_plus_dt\n",
    "            angular_vel[i, :] = delta_rot.as_rotvec() / time_delta\n",
    "        except ValueError: pass\n",
    "    return angular_vel\n",
    "\n",
    "def calculate_angular_distance(rot_data):\n",
    "    quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    angular_dist = np.zeros(len(quat_values))\n",
    "    for i in range(len(quat_values) - 1):\n",
    "        q1, q2 = quat_values[i], quat_values[i+1]\n",
    "        if np.all(np.isnan(q1)) or np.all(np.isnan(q2)): continue\n",
    "        try:\n",
    "            r1, r2 = R.from_quat(q1), R.from_quat(q2)\n",
    "            relative_rotation = r1.inv() * r2\n",
    "            angular_dist[i] = np.linalg.norm(relative_rotation.as_rotvec())\n",
    "        except ValueError: pass\n",
    "    return angular_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5787b059",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T00:47:14.136141Z",
     "iopub.status.busy": "2025-06-26T00:47:14.135894Z",
     "iopub.status.idle": "2025-06-26T00:47:14.143692Z",
     "shell.execute_reply": "2025-06-26T00:47:14.143149Z"
    },
    "papermill": {
     "duration": 0.012356,
     "end_time": "2025-06-26T00:47:14.144832",
     "exception": false,
     "start_time": "2025-06-26T00:47:14.132476",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Tensor Manipulations\n",
    "def time_sum(x): return K.sum(x, axis=1)\n",
    "def squeeze_last_axis(x): return tf.squeeze(x, axis=-1)\n",
    "def expand_last_axis(x): return tf.expand_dims(x, axis=-1)\n",
    "\n",
    "def se_block(x, reduction=8):\n",
    "    ch = x.shape[-1]\n",
    "    se = GlobalAveragePooling1D()(x)\n",
    "    se = Dense(ch // reduction, activation='relu')(se)\n",
    "    se = Dense(ch, activation='sigmoid')(se)\n",
    "    se = Reshape((1, ch))(se)\n",
    "    return Multiply()([x, se])\n",
    "\n",
    "def residual_se_cnn_block(x, filters, kernel_size, pool_size=2, drop=0.3, wd=1e-4):\n",
    "    shortcut = x\n",
    "    for _ in range(2):\n",
    "        x = Conv1D(filters, kernel_size, padding='same', use_bias=False,\n",
    "                   kernel_regularizer=l2(wd))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "    x = se_block(x)\n",
    "    if shortcut.shape[-1] != filters:\n",
    "        shortcut = Conv1D(filters, 1, padding='same', use_bias=False,\n",
    "                          kernel_regularizer=l2(wd))(shortcut)\n",
    "        shortcut = BatchNormalization()(shortcut)\n",
    "    x = add([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling1D(pool_size)(x)\n",
    "    x = Dropout(drop)(x)\n",
    "    return x\n",
    "\n",
    "def attention_layer(inputs):\n",
    "    score = Dense(1, activation='tanh')(inputs)\n",
    "    score = Lambda(squeeze_last_axis)(score)\n",
    "    weights = Activation('softmax')(score)\n",
    "    weights = Lambda(expand_last_axis)(weights)\n",
    "    context = Multiply()([inputs, weights])\n",
    "    context = Lambda(time_sum)(context)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f956ef28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T00:47:14.151204Z",
     "iopub.status.busy": "2025-06-26T00:47:14.150636Z",
     "iopub.status.idle": "2025-06-26T00:47:14.158900Z",
     "shell.execute_reply": "2025-06-26T00:47:14.158134Z"
    },
    "papermill": {
     "duration": 0.012488,
     "end_time": "2025-06-26T00:47:14.160072",
     "exception": false,
     "start_time": "2025-06-26T00:47:14.147584",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GatedMixupGenerator(Sequence):\n",
    "    def __init__(self, X, y, batch_size, imu_dim, class_weight=None, alpha=0.2, masking_prob=0.0):\n",
    "        self.X, self.y = X, y\n",
    "        self.batch = batch_size\n",
    "        self.imu_dim = imu_dim\n",
    "        self.class_weight = class_weight\n",
    "        self.alpha = alpha\n",
    "        self.masking_prob = masking_prob\n",
    "        self.indices = np.arange(len(X))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.X) / self.batch))\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        idx = self.indices[i*self.batch:(i+1)*self.batch]\n",
    "        Xb, yb = self.X[idx].copy(), self.y[idx].copy()\n",
    "        \n",
    "        # サンプルごとの重みを計算\n",
    "        sample_weights = np.ones(len(Xb), dtype='float32')\n",
    "        if self.class_weight:\n",
    "            y_integers = yb.argmax(axis=1)\n",
    "            sample_weights = np.array([self.class_weight[i] for i in y_integers])\n",
    "        \n",
    "        gate_target = np.ones(len(Xb), dtype='float32')\n",
    "        if self.masking_prob > 0:\n",
    "            for i in range(len(Xb)):\n",
    "                if np.random.rand() < self.masking_prob:\n",
    "                    Xb[i, :, self.imu_dim:] = 0\n",
    "                    gate_target[i] = 0.0\n",
    "\n",
    "        if self.alpha > 0:\n",
    "            lam = np.random.beta(self.alpha, self.alpha)\n",
    "            perm = np.random.permutation(len(Xb))\n",
    "            X_mix = lam * Xb + (1 - lam) * Xb[perm]\n",
    "            y_mix = lam * yb + (1 - lam) * yb[perm]\n",
    "            gate_target_mix = lam * gate_target + (1 - lam) * gate_target[perm]\n",
    "            sample_weights_mix = lam * sample_weights + (1 - lam) * sample_weights[perm]\n",
    "            return X_mix, {'main_output': y_mix, 'tof_gate': gate_target_mix}, sample_weights_mix\n",
    "\n",
    "        return Xb, {'main_output': yb, 'tof_gate': gate_target}, sample_weights\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2c01b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.nn_blocks import unet_se_cnn\n",
    "\n",
    "def build_gated_two_branch_model(pad_len, imu_dim, tof_dim, n_classes, wd=1e-4):\n",
    "    inp = Input(shape=(pad_len, imu_dim+tof_dim))\n",
    "    imu = Lambda(lambda t: t[:, :, :imu_dim])(inp)\n",
    "    tof = Lambda(lambda t: t[:, :, imu_dim:])(inp)\n",
    "\n",
    "    x1 = unet_se_cnn(imu, 32, 2, drop=0.1)#, wd=wd)\n",
    "\n",
    "    x2_base = Conv1D(16, 3, padding='same', use_bias=False, kernel_regularizer=l2(wd))(tof)\n",
    "    x2_base = BatchNormalization()(x2_base); x2_base = Activation('relu')(x2_base)\n",
    "    x2_base = MaxPooling1D(2)(x2_base); x2_base = Dropout(0.2)(x2_base)\n",
    "    x2_base = Conv1D(32, 3, padding='same', use_bias=False, kernel_regularizer=l2(wd))(x2_base)\n",
    "    x2_base = BatchNormalization()(x2_base); x2_base = Activation('relu')(x2_base)\n",
    "    x2_base = MaxPooling1D(2)(x2_base); x2_base = Dropout(0.2)(x2_base)\n",
    "    \n",
    "    gate_input = GlobalAveragePooling1D()(tof)\n",
    "    gate_input = Dense(16, activation='relu')(gate_input)\n",
    "    gate = Dense(1, activation='sigmoid', name='tof_gate')(gate_input)\n",
    "    \n",
    "    x2 = Multiply()([x2_base, gate])\n",
    "\n",
    "    merged = Concatenate()([x1, x2])\n",
    "    xa = Bidirectional(LSTM(32, return_sequences=True, kernel_regularizer=l2(wd)))(merged)\n",
    "    xb = Bidirectional(GRU(32, return_sequences=True, kernel_regularizer=l2(wd)))(merged)\n",
    "    xc = GaussianNoise(0.09)(merged)\n",
    "    xc = Dense(16, activation='elu')(xc)\n",
    "    x = Concatenate()([xa, xb, xc])\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = attention_layer(x)\n",
    "    for units, drop in [(129, 0.5), (32, 0.3)]:\n",
    "        x = Dense(units, use_bias=False, kernel_regularizer=l2(wd))(x)\n",
    "        x = BatchNormalization()(x); x = Activation('relu')(x)\n",
    "        x = Dropout(drop)(x)\n",
    "\n",
    "    out = Dense(n_classes, activation='softmax', name='main_output', kernel_regularizer=l2(wd))(x)\n",
    "    \n",
    "    return Model(inputs=inp, outputs=[out, gate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c510be54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T00:47:14.166170Z",
     "iopub.status.busy": "2025-06-26T00:47:14.165912Z",
     "iopub.status.idle": "2025-06-26T00:47:14.174527Z",
     "shell.execute_reply": "2025-06-26T00:47:14.173948Z"
    },
    "papermill": {
     "duration": 0.013012,
     "end_time": "2025-06-26T00:47:14.175638",
     "exception": false,
     "start_time": "2025-06-26T00:47:14.162626",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_gated_two_branch_model(pad_len, imu_dim, tof_dim, n_classes, wd=1e-4):\n",
    "    inp = Input(shape=(pad_len, imu_dim+tof_dim))\n",
    "    imu = Lambda(lambda t: t[:, :, :imu_dim])(inp)\n",
    "    tof = Lambda(lambda t: t[:, :, imu_dim:])(inp)\n",
    "\n",
    "    x1 = residual_se_cnn_block(imu, 64, 3, drop=0.1, wd=wd)\n",
    "    x1 = residual_se_cnn_block(x1, 128, 5, drop=0.1, wd=wd)\n",
    "\n",
    "    x2_base = Conv1D(64, 3, padding='same', use_bias=False, kernel_regularizer=l2(wd))(tof)\n",
    "    x2_base = BatchNormalization()(x2_base); x2_base = Activation('relu')(x2_base)\n",
    "    x2_base = MaxPooling1D(2)(x2_base); x2_base = Dropout(0.2)(x2_base)\n",
    "    x2_base = Conv1D(128, 3, padding='same', use_bias=False, kernel_regularizer=l2(wd))(x2_base)\n",
    "    x2_base = BatchNormalization()(x2_base); x2_base = Activation('relu')(x2_base)\n",
    "    x2_base = MaxPooling1D(2)(x2_base); x2_base = Dropout(0.2)(x2_base)\n",
    "    \n",
    "    gate_input = GlobalAveragePooling1D()(tof)\n",
    "    gate_input = Dense(16, activation='relu')(gate_input)\n",
    "    # 補助損失の計算対象となるため、レイヤーに名前を付ける\n",
    "    gate = Dense(1, activation='sigmoid', name='tof_gate')(gate_input)\n",
    "    \n",
    "    x2 = Multiply()([x2_base, gate])\n",
    "\n",
    "    merged = Concatenate()([x1, x2])\n",
    "    xa = Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer=l2(wd)))(merged)\n",
    "    xb = Bidirectional(GRU(128, return_sequences=True, kernel_regularizer=l2(wd)))(merged)\n",
    "    xc = GaussianNoise(0.09)(merged)\n",
    "    xc = Dense(16, activation='elu')(xc)\n",
    "    x = Concatenate()([xa, xb, xc])\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = attention_layer(x)\n",
    "    for units, drop in [(256, 0.5), (128, 0.3)]:\n",
    "        x = Dense(units, use_bias=False, kernel_regularizer=l2(wd))(x)\n",
    "        x = BatchNormalization()(x); x = Activation('relu')(x)\n",
    "        x = Dropout(drop)(x)\n",
    "\n",
    "    out = Dense(n_classes, activation='softmax', name='main_output', kernel_regularizer=l2(wd))(x)\n",
    "\n",
    "    return Model(inputs=inp, outputs=[out, gate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb7c6809",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T00:47:14.181930Z",
     "iopub.status.busy": "2025-06-26T00:47:14.181736Z",
     "iopub.status.idle": "2025-06-26T00:47:20.480614Z",
     "shell.execute_reply": "2025-06-26T00:47:20.479638Z"
    },
    "papermill": {
     "duration": 6.303715,
     "end_time": "2025-06-26T00:47:20.481952",
     "exception": false,
     "start_time": "2025-06-26T00:47:14.178237",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if DEBUG_GATE and not TRAIN:\n",
    "#     print(\"▶ GATE DEBUG MODE – preparing data to analyze trained models...\")\n",
    "    \n",
    "#     # --- 1. 学習時と全く同じデータ準備プロセスを実行 ---\n",
    "#     df = pd.read_csv(RAW_DIR / \"train.csv\")\n",
    "#     train_dem_df = pd.read_csv(RAW_DIR / \"train_demographics.csv\")\n",
    "#     df = pd.merge(df, train_dem_df, on='subject', how='left')\n",
    "#     le = LabelEncoder()\n",
    "#     df['gesture_int'] = le.fit_transform(df['gesture'])\n",
    "#     print(\"  Calculating engineered features...\")\n",
    "#     df['acc_mag'] = np.sqrt(df['acc_x']**2 + df['acc_y']**2 + df['acc_z']**2)\n",
    "#     df['rot_angle'] = 2 * np.arccos(df['rot_w'].clip(-1, 1))\n",
    "#     df['acc_mag_jerk'] = df.groupby('sequence_id')['acc_mag'].diff().fillna(0)\n",
    "#     df['rot_angle_vel'] = df.groupby('sequence_id')['rot_angle'].diff().fillna(0)\n",
    "#     cols_for_stats = ['acc_mag', 'rot_angle', 'acc_mag_jerk', 'rot_angle_vel']\n",
    "#     for col in cols_for_stats:\n",
    "#         df[f'{col}_skew'] = df.groupby('sequence_id')[col].transform('skew')\n",
    "#         df[f'{col}_kurt'] = df.groupby('sequence_id')[col].transform(pd.Series.kurtosis)\n",
    "    \n",
    "#     # --- [修正点] 成果物の読み込み元から特徴量リストを取得 ---\n",
    "#     final_feature_cols = np.load(PRETRAINED_DIR / \"feature_cols.npy\", allow_pickle=True).tolist()\n",
    "    \n",
    "#     print(\"  Building sequences...\")\n",
    "#     seq_gp = df.groupby('sequence_id')\n",
    "#     X_list_unscaled, y_list_int, groups_list, lens = [], [], [], []\n",
    "#     for seq_id, seq_df in seq_gp:\n",
    "#         seq_df_copy = seq_df.copy()\n",
    "#         for i in range(1, 6):\n",
    "#             pixel_cols_tof = [f\"tof_{i}_v{p}\" for p in range(64)]\n",
    "#             tof_sensor_data = seq_df_copy[pixel_cols_tof].replace(-1, np.nan)\n",
    "#             seq_df_copy[f'tof_{i}_mean'] = tof_sensor_data.mean(axis=1)\n",
    "#             seq_df_copy[f'tof_{i}_std']  = tof_sensor_data.std(axis=1)\n",
    "#             seq_df_copy[f'tof_{i}_min']  = tof_sensor_data.min(axis=1)\n",
    "#             seq_df_copy[f'tof_{i}_max']  = tof_sensor_data.max(axis=1)\n",
    "#         mat_unscaled = seq_df_copy[final_feature_cols].ffill().bfill().fillna(0).values.astype('float32')\n",
    "#         X_list_unscaled.append(mat_unscaled)\n",
    "#         y_list_int.append(seq_df_copy['gesture_int'].iloc[0])\n",
    "#         groups_list.append(seq_df_copy['subject'].iloc[0])\n",
    "#         lens.append(len(mat_unscaled))\n",
    "\n",
    "#     print(\"  Loading scaler and padding sequences...\")\n",
    "#     # --- [修正点] 読み込み元を PRETRAINED_DIR に変更 ---\n",
    "#     scaler = joblib.load(PRETRAINED_DIR / \"scaler.pkl\") \n",
    "#     pad_len = int(np.load(PRETRAINED_DIR / \"sequence_maxlen.npy\"))\n",
    "    \n",
    "#     X_scaled_list = [scaler.transform(x_seq) for x_seq in X_list_unscaled]\n",
    "#     del X_list_unscaled\n",
    "#     X = pad_sequences(X_scaled_list, maxlen=pad_len, padding='post', truncating='post', dtype='float32')\n",
    "#     del X_scaled_list\n",
    "#     y_stratify = np.array(y_list_int)\n",
    "#     groups = np.array(groups_list)\n",
    "\n",
    "#     # --- 2. CV分割を再現し、各フォールドのモデルを分析 ---\n",
    "#     sgkf = StratifiedGroupKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "#     custom_objs = {\n",
    "#         'time_sum': time_sum, 'squeeze_last_axis': squeeze_last_axis, 'expand_last_axis': expand_last_axis,\n",
    "#         'se_block': se_block, 'residual_se_cnn_block': residual_se_cnn_block, 'attention_layer': attention_layer,\n",
    "#     }\n",
    "\n",
    "#     for fold, (train_idx, val_idx) in enumerate(sgkf.split(X, y_stratify, groups)):\n",
    "#         print(f\"\\n===== Analyzing FOLD {fold+1}/{N_SPLITS} =====\")\n",
    "#         X_val = X[val_idx]\n",
    "        \n",
    "#         # --- [修正点] 読み込み元を PRETRAINED_DIR に変更 ---\n",
    "#         model_path = PRETRAINED_DIR / f\"gesture_model_fold_{fold}.h5\"\n",
    "#         if not model_path.exists():\n",
    "#             print(f\"Model file not found: {model_path}. Skipping this fold.\")\n",
    "#             continue\n",
    "#         model = load_model(model_path, compile=False, custom_objects=custom_objs)\n",
    "\n",
    "#         if 'tof_gate' not in [layer.name for layer in model.layers]:\n",
    "#             print(\"Error: 'tof_gate' layer not found in the model.\")\n",
    "#             break\n",
    "#         gate_output = model.get_layer('tof_gate').output\n",
    "#         debug_model = Model(inputs=model.input, outputs=[model.output, gate_output])\n",
    "\n",
    "#         print(\"  Predicting on validation data to get gate values...\")\n",
    "#         _, gate_values = debug_model.predict(X_val, batch_size=64) # BATCH_SIZE\n",
    "#         gate_values = gate_values.flatten()\n",
    "\n",
    "#         print(f\"  Gate Value Stats: Mean={np.mean(gate_values):.4f}, Std={np.std(gate_values):.4f}, Min={np.min(gate_values):.4f}, Max={np.max(gate_values):.4f}\")\n",
    "\n",
    "#         plt.figure(figsize=(10, 5))\n",
    "#         plt.hist(gate_values, bins=50, range=(0, 1), color='skyblue', edgecolor='black')\n",
    "#         plt.title(f\"Fold {fold+1} Gate Value Distribution\", fontsize=16)\n",
    "#         plt.xlabel(\"Gate Value (0 = TOF/THM Off, 1 = TOF/THM On)\", fontsize=12)\n",
    "#         plt.ylabel(\"Frequency\", fontsize=12)\n",
    "#         plt.grid(axis='y', alpha=0.75)\n",
    "#         plt.show()\n",
    "    \n",
    "#     print(\"\\n✔ Gate analysis finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a47c77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ TRAIN MODE – loading dataset …\n",
      "  Removing gravity and calculating linear acceleration features...\n",
      "  Calculating angular velocity and distance from quaternions...\n"
     ]
    }
   ],
   "source": [
    "print(\"▶ TRAIN MODE – loading dataset …\")\n",
    "df = pd.read_csv(RAW_DIR / \"train.csv\")\n",
    "train_dem_df = pd.read_csv(RAW_DIR / \"train_demographics.csv\")\n",
    "df = pd.merge(df, train_dem_df, on='subject', how='left')\n",
    "le = LabelEncoder()\n",
    "df['gesture_int'] = le.fit_transform(df['gesture'])\n",
    "np.save(EXPORT_DIR / \"gesture_classes.npy\", le.classes_)\n",
    "\n",
    "# --- [変更点] 物理FEの導入 ---\n",
    "print(\"  Removing gravity and calculating linear acceleration features...\")\n",
    "linear_accel_list = [pd.DataFrame(remove_gravity_from_acc(group[['acc_x', 'acc_y', 'acc_z']], group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]), columns=['linear_acc_x', 'linear_acc_y', 'linear_acc_z'], index=group.index) for _, group in df.groupby('sequence_id')]\n",
    "df = pd.concat([df, pd.concat(linear_accel_list)], axis=1)\n",
    "df['linear_acc_mag'] = np.sqrt(df['linear_acc_x']**2 + df['linear_acc_y']**2 + df['linear_acc_z']**2)\n",
    "df['linear_acc_mag_jerk'] = df.groupby('sequence_id')['linear_acc_mag'].diff().fillna(0)\n",
    "\n",
    "print(\"  Calculating angular velocity and distance from quaternions...\")\n",
    "angular_vel_list = [pd.DataFrame(calculate_angular_velocity_from_quat(group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]), columns=['angular_vel_x', 'angular_vel_y', 'angular_vel_z'], index=group.index) for _, group in df.groupby('sequence_id')]\n",
    "df = pd.concat([df, pd.concat(angular_vel_list)], axis=1)\n",
    "angular_dist_list = [pd.DataFrame(calculate_angular_distance(group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]), columns=['angular_distance'], index=group.index) for _, group in df.groupby('sequence_id')]\n",
    "df = pd.concat([df, pd.concat(angular_dist_list)], axis=1)\n",
    "\n",
    "# --- [変更点] 物理FEを反映した特徴量リスト ---\n",
    "imu_cols_base = ['linear_acc_x', 'linear_acc_y', 'linear_acc_z'] + [c for c in df.columns if c.startswith('rot_')]\n",
    "imu_engineered = ['linear_acc_mag', 'linear_acc_mag_jerk', 'angular_vel_x', 'angular_vel_y', 'angular_vel_z', 'angular_distance']\n",
    "imu_cols = list(dict.fromkeys(imu_cols_base + imu_engineered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f68736c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  IMU (phys-based) 13 | THM + Aggregated TOF 25 | total 38 features\n"
     ]
    }
   ],
   "source": [
    "thm_cols_original = [c for c in df.columns if c.startswith('thm_')]\n",
    "tof_aggregated_cols_template = []\n",
    "for i in range(1, 6): tof_aggregated_cols_template.extend([f'tof_{i}_mean', f'tof_{i}_std', f'tof_{i}_min', f'tof_{i}_max'])\n",
    "\n",
    "final_feature_cols = imu_cols + thm_cols_original + tof_aggregated_cols_template\n",
    "imu_dim_final = len(imu_cols)\n",
    "tof_thm_aggregated_dim_final = len(thm_cols_original) + len(tof_aggregated_cols_template)\n",
    "\n",
    "print(f\"  IMU (phys-based) {imu_dim_final} | THM + Aggregated TOF {tof_thm_aggregated_dim_final} | total {len(final_feature_cols)} features\")\n",
    "np.save(EXPORT_DIR / \"feature_cols.npy\", np.array(final_feature_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a94ed88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building sequences...\n"
     ]
    }
   ],
   "source": [
    "print(\"  Building sequences...\")\n",
    "seq_gp = df.groupby('sequence_id') \n",
    "X_list_unscaled, y_list_int, groups_list, lens = [], [], [], [] \n",
    "for seq_id, seq_df in seq_gp:\n",
    "    seq_df_copy = seq_df.copy()\n",
    "    for i in range(1, 6):\n",
    "        pixel_cols = [f\"tof_{i}_v{p}\" for p in range(64)]; tof_data = seq_df_copy[pixel_cols].replace(-1, np.nan)\n",
    "        seq_df_copy[f'tof_{i}_mean'], seq_df_copy[f'tof_{i}_std'], seq_df_copy[f'tof_{i}_min'], seq_df_copy[f'tof_{i}_max'] = tof_data.mean(axis=1), tof_data.std(axis=1), tof_data.min(axis=1), tof_data.max(axis=1)\n",
    "    X_list_unscaled.append(seq_df_copy[final_feature_cols].ffill().bfill().fillna(0).values.astype('float32'))\n",
    "    y_list_int.append(seq_df_copy['gesture_int'].iloc[0])\n",
    "    groups_list.append(seq_df_copy['subject'].iloc[0])\n",
    "    lens.append(len(seq_df_copy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a1447cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting StandardScaler...\n",
      "  Scaling and padding sequences...\n"
     ]
    }
   ],
   "source": [
    "print(\"  Fitting StandardScaler...\")\n",
    "all_steps_concatenated = np.concatenate(X_list_unscaled, axis=0)\n",
    "scaler = StandardScaler().fit(all_steps_concatenated)\n",
    "joblib.dump(scaler, EXPORT_DIR / \"scaler.pkl\")\n",
    "\n",
    "print(\"  Scaling and padding sequences...\")\n",
    "X_scaled_list = [scaler.transform(x_seq) for x_seq in X_list_unscaled]\n",
    "pad_len = int(np.percentile(lens, PAD_PERCENTILE)); np.save(EXPORT_DIR / \"sequence_maxlen.npy\", pad_len)\n",
    "X = pad_sequences(X_scaled_list, maxlen=pad_len, padding='post', truncating='post', dtype='float32')\n",
    "y_stratify, groups, y = np.array(y_list_int), np.array(groups_list), to_categorical(y_list_int, num_classes=len(le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f322d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Starting training with Stratified Group K-Fold CV...\n",
      "\n",
      "===== FOLD 1/5 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1754685456.886735   29313 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4416 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1060, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✔ Training done.\n",
      "Overall OOF H-F1 Score = 0.3935\n"
     ]
    }
   ],
   "source": [
    "print(\"  Starting training with Stratified Group K-Fold CV...\")\n",
    "sgkf = StratifiedGroupKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "oof_preds = np.zeros_like(y, dtype='float32')\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(sgkf.split(X, y_stratify, groups)):\n",
    "    print(f\"\\n===== FOLD {fold+1}/{N_SPLITS} =====\")\n",
    "    X_tr, X_val, y_tr, y_val = X[train_idx], X[val_idx], y[train_idx], y[val_idx]\n",
    "    model = build_gated_two_branch_model(pad_len, imu_dim_final, tof_thm_aggregated_dim_final, len(le.classes_), wd=WD)\n",
    "    model.compile(optimizer=Adam(LR_INIT),\n",
    "                    loss={'main_output': tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1), 'tof_gate': tf.keras.losses.BinaryCrossentropy()},\n",
    "                    loss_weights={'main_output': 1.0, 'tof_gate': GATE_LOSS_WEIGHT},\n",
    "                    metrics={'main_output': 'accuracy'})\n",
    "    class_weight_dict = dict(enumerate(compute_class_weight('balanced', classes=np.arange(len(le.classes_)), y=y_tr.argmax(1))))\n",
    "    train_gen = GatedMixupGenerator(X_tr, y_tr, batch_size=BATCH_SIZE, imu_dim=imu_dim_final, class_weight=class_weight_dict, alpha=MIXUP_ALPHA, masking_prob=MASKING_PROB)\n",
    "    val_gen = GatedMixupGenerator(X_val, y_val, batch_size=BATCH_SIZE, imu_dim=imu_dim_final)\n",
    "    cb = EarlyStopping(patience=PATIENCE, restore_best_weights=True, verbose=1, monitor='val_main_output_accuracy', mode='max')\n",
    "    break\n",
    "    model.fit(train_gen, epochs=EPOCHS, validation_data=val_gen, callbacks=[cb], verbose=1)\n",
    "    model.save(EXPORT_DIR / f\"gesture_model_fold_{fold}.h5\")\n",
    "    preds_val, _ = model.predict(X_val); oof_preds[val_idx] = preds_val\n",
    "\n",
    "# (OOFスコア計算部分はcmi_2025_metric_copy_for_import.pyがあれば有効化)\n",
    "print(\"\\n✔ Training done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d0b62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 評価指標ファイルをインポートしてOOFスコアを計算\n",
    "from src.metric import CompetitionMetric\n",
    "true_oof_int = y.argmax(1)\n",
    "pred_oof_int = oof_preds.argmax(1)\n",
    "\n",
    "h_f1_oof = CompetitionMetric().calculate_hierarchical_f1(\n",
    "    pd.DataFrame({'gesture': le.classes_[true_oof_int]}),\n",
    "    pd.DataFrame({'gesture': le.classes_[pred_oof_int]}))\n",
    "print(f\"Overall OOF H-F1 Score = {h_f1_oof:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb80789a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# else:\n",
    "print(\"▶ INFERENCE MODE – loading artefacts from\", PRETRAINED_DIR)\n",
    "final_feature_cols = np.load(PRETRAINED_DIR / \"feature_cols.npy\", allow_pickle=True).tolist()\n",
    "pad_len        = int(np.load(PRETRAINED_DIR / \"sequence_maxlen.npy\"))\n",
    "scaler         = joblib.load(PRETRAINED_DIR / \"scaler.pkl\")\n",
    "gesture_classes = np.load(PRETRAINED_DIR / \"gesture_classes.npy\", allow_pickle=True)\n",
    "\n",
    "# Note: imu_dim_final and tof_thm_aggregated_dim_final are not needed for inference here\n",
    "# as the predict function works on the full feature set.\n",
    "\n",
    "custom_objs = {\n",
    "    'time_sum': time_sum, 'squeeze_last_axis': squeeze_last_axis, 'expand_last_axis': expand_last_axis,\n",
    "    'se_block': se_block, 'residual_se_cnn_block': residual_se_cnn_block, 'attention_layer': attention_layer,\n",
    "}\n",
    "\n",
    "models = []\n",
    "print(f\"  Loading {N_SPLITS} models for ensemble inference...\")\n",
    "for fold in range(N_SPLITS):\n",
    "    # --- [重要] 推論時にはカスタムオブジェクトに`build_gated_two_branch_model`は不要 ---\n",
    "    # 保存されたモデルはレイヤーの構造を保持しているため、カスタムレイヤー/関数のみ渡せばよい\n",
    "    model_path = PRETRAINED_DIR / f\"gesture_model_fold_{fold}.h5\"\n",
    "    model = load_model(model_path, compile=False, custom_objects=custom_objs)\n",
    "    models.append(model)\n",
    "print(\"  Models, scaler, feature_cols, pad_len loaded – ready for evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ec7514",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T00:47:20.489575Z",
     "iopub.status.busy": "2025-06-26T00:47:20.489327Z",
     "iopub.status.idle": "2025-06-26T00:47:20.499513Z",
     "shell.execute_reply": "2025-06-26T00:47:20.498876Z"
    },
    "papermill": {
     "duration": 0.015299,
     "end_time": "2025-06-26T00:47:20.500662",
     "exception": false,
     "start_time": "2025-06-26T00:47:20.485363",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- [新機能] TTA用のハイパーパラメータ ---\n",
    "TTA_STEPS = 10  # TTAの実行回数。5〜10あたりが一般的。\n",
    "TTA_NOISE_STDDEV = 0.01 # 入力データに加えるノイズの標準偏差\n",
    "\n",
    "# --- [変更点] TTAを実装したpredict関数 ---\n",
    "def predict(sequence: pl.DataFrame, demographics: pl.DataFrame) -> str:\n",
    "    df_seq = sequence.to_pandas()\n",
    "    \n",
    "    # --- 1. 特徴量エンジニアリング（変更なし） ---\n",
    "    linear_accel = remove_gravity_from_acc(df_seq, df_seq)\n",
    "    df_seq['linear_acc_x'], df_seq['linear_acc_y'], df_seq['linear_acc_z'] = linear_accel[:, 0], linear_accel[:, 1], linear_accel[:, 2]\n",
    "    df_seq['linear_acc_mag'] = np.sqrt(df_seq['linear_acc_x']**2 + df_seq['linear_acc_y']**2 + df_seq['linear_acc_z']**2)\n",
    "    df_seq['linear_acc_mag_jerk'] = df_seq['linear_acc_mag'].diff().fillna(0)\n",
    "    angular_vel = calculate_angular_velocity_from_quat(df_seq)\n",
    "    df_seq['angular_vel_x'], df_seq['angular_vel_y'], df_seq['angular_vel_z'] = angular_vel[:, 0], angular_vel[:, 1], angular_vel[:, 2]\n",
    "    df_seq['angular_distance'] = calculate_angular_distance(df_seq)\n",
    "    for i in range(1, 6):\n",
    "        pixel_cols = [f\"tof_{i}_v{p}\" for p in range(64)]; tof_data = df_seq[pixel_cols].replace(-1, np.nan)\n",
    "        df_seq[f'tof_{i}_mean'], df_seq[f'tof_{i}_std'], df_seq[f'tof_{i}_min'], df_seq[f'tof_{i}_max'] = tof_data.mean(axis=1), tof_data.std(axis=1), tof_data.min(axis=1), tof_data.max(axis=1)\n",
    "        \n",
    "    mat_unscaled = df_seq[final_feature_cols].ffill().bfill().fillna(0).values.astype('float32')\n",
    "    mat_scaled = scaler.transform(mat_unscaled)\n",
    "    pad_input = pad_sequences([mat_scaled], maxlen=pad_len, padding='post', truncating='post', dtype='float32')\n",
    "\n",
    "    # --- 2. TTAループの実行 ---\n",
    "    all_tta_predictions = []\n",
    "    for _ in range(TTA_STEPS):\n",
    "        # 元の入力データにノイズを加える (TTA_STEPS=1回目はノイズなしでも良い)\n",
    "        if TTA_STEPS > 1 and _ > 0: # 最初の1回は元のデータで予測\n",
    "             noisy_input = pad_input + tf.random.normal(shape=tf.shape(pad_input), mean=0.0, stddev=TTA_NOISE_STDDEV)\n",
    "        else:\n",
    "             noisy_input = pad_input\n",
    "\n",
    "        # 5フォールドモデルでアンサンブル予測\n",
    "        all_fold_predictions = []\n",
    "        for model in models:\n",
    "            # 主出力(ジェスチャー確率)のみを取得\n",
    "            main_preds, _ = model.predict(noisy_input, verbose=0)\n",
    "            all_fold_predictions.append(main_preds)\n",
    "        \n",
    "        # フォールド間の予測を平均\n",
    "        avg_fold_prediction = np.mean(all_fold_predictions, axis=0)\n",
    "        all_tta_predictions.append(avg_fold_prediction)\n",
    "\n",
    "    # --- 3. TTAの結果を最終的に平均化 ---\n",
    "    final_avg_prediction = np.mean(all_tta_predictions, axis=0)\n",
    "    \n",
    "    # 最も確率の高いクラスのインデックスを取得\n",
    "    idx = int(final_avg_prediction.argmax())\n",
    "    \n",
    "    return str(gesture_classes[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a39861a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T00:47:20.507777Z",
     "iopub.status.busy": "2025-06-26T00:47:20.507529Z",
     "iopub.status.idle": "2025-06-26T00:47:37.087561Z",
     "shell.execute_reply": "2025-06-26T00:47:37.086753Z"
    },
    "papermill": {
     "duration": 16.585159,
     "end_time": "2025-06-26T00:47:37.088992",
     "exception": false,
     "start_time": "2025-06-26T00:47:20.503833",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 00:47:21.722414: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n",
      "I0000 00:00:1750898842.782712      60 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2025-06-26 00:47:27.113142: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n",
      "2025-06-26 00:47:32.622484: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n"
     ]
    }
   ],
   "source": [
    "if not TRAIN:\n",
    "    import kaggle_evaluation.cmi_inference_server\n",
    "    inference_server = kaggle_evaluation.cmi_inference_server.CMIInferenceServer(predict)\n",
    "\n",
    "    if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "        inference_server.serve()\n",
    "    else:\n",
    "        inference_server.run_local_gateway(\n",
    "            data_paths=(\n",
    "                '/kaggle/input/cmi-detect-behavior-with-sensor-data/test.csv',\n",
    "                '/kaggle/input/cmi-detect-behavior-with-sensor-data/test_demographics.csv',\n",
    "            )\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 12518947,
     "sourceId": 102335,
     "sourceType": "competition"
    },
    {
     "datasetId": 7713851,
     "sourceId": 12274546,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 242954653,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "cmi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 48.360681,
   "end_time": "2025-06-26T00:47:40.058594",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-26T00:46:51.697913",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
